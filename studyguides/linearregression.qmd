---
title: Introduction to linear regression
author: Flora Green
abstract-title: Summary
abstract: Linear regression is a statistical model which uses one or more variables to predict the behaviour of another. It is widely used in fields such as business, economics and social sciences.
categories:
  - Statistics
image: FiguresPNG/linearregression-image.png
filters:
  - shinylive
draft: true
---

```{r, setup, include = FALSE}
library("webexercises")
```

*Before reading this guide, it is strongly recommended that you read [Guide: Introduction to straight lines] and [Guide: Expected value, variance, standard deviation](expectedvariance.qmd).*

::: {.content-visible when-format="html"}

```{=html}
<table><tr><td style="vertical-align: middle"><strong>Narration of study guide:</strong>&nbsp;&nbsp;</td><td><audio controls><source src="./Narrations/hypothesistesting.mp3" type="audio/mpeg">Your browser does not support the audio element.</audio></tr></table>
```

:::

# What is linear regression? {-}

One of the key purposes of statistics is the interpretation of data, which often involves considering the relationships between variables. Typically, this information is split into two categories: 'the thing which is observed' and 'the thing(s) which affect this observation', and a statistician collects information in both categories to build a full picture of the scenario. Then, the statistician tries to explain how and why a certain outcome is reached - which is incredibly useful for predicting which outcome may be reached in the future!

For instance, if the seaside branch of Cantor's Confectionery had 150 ice cream sales in one day, and only 30 in another, they may wish to understand what it was that motivated this change, so that they can have the appropriate number of ice creams in stock. It could be that one of these days was a warm, sunny, summer's Saturday, whilst the other was a cold, rainy winter's Monday. If you take a sample of the weather, season and day of the week and the corresponding number of ice cream sales across multiple days, you can build a model which allows you to predict the number of ice cream sales on a particular day.

One of the models which describes the relationship between variables is known as **linear regression**. Linear regression finds a line of best fit for your data, known as a **regression line**, which can be used to predict outcomes - like the number of ice cream sales.

This guide will introduce you to linear regression. It will explain how to apply a simple linear regression model to a sample of data by finding the regression line. It will also explore the practical applications of linear regression, including in the construction of confidence intervals which encapsulate the uncertainty about how closely your sample matches the real-world scenario, and in testing hypotheses about whether some factors impact the overall outcome.

# Simple linear regression

::: {.callout-note appearance="simple"}

## Definition of simple linear regression

**Simple linear regression** is a linear regression model used when there is only one explanatory variable, $x$. The **regression line** predicts the value of the response variable, which is expressed as $\mathbb{E}(Y)$. 

For more on expected values, please see [Guide: Expected value, variance, standard deviation](expectedvariance.qmd). For more about explanatory and response variables, please see [Overview: Statistical Notation](../overviews/o-notation.qmd).

:::

To find the equation of the regression line for a population of data, you will need to find the y-intercept of the line, which will be denoted by $\alpha$, and its gradient, which will be described as $\beta$. 

For more about the equation of a straight line, please read [Guide: Introduction to straight lines].

Therefore the equation of the regression line is given by:
$$\mathbb{E}(Y) = \alpha + \beta x$$

In practice, it is impractical to work with an entire population of data - instead, statisticians analyze samples. This means that you will need to obtain **estimates** for the values of $\alpha$ and $\beta$ from your sample of data, and these will be denoted by $\hat{\alpha}$ and $\hat{\beta}$.

:::{.callout-tip}

$\hat{\alpha}$ and  $\hat{\beta}$ are **unbiased** estimators of the population **regression parameters** $\alpha$ and $\beta$, which means that their long-term averages are the same.

For more about biased estimators, please read [Guide: Biased and unbiased estimators].

:::

::: {.callout-note appearance="simple"}
## Example 1

The seaside branch of Cantor's Confectionery wishes to see the relationship between temperature ($x$) and ice cream sales ($y$). To do this, they track the number of ice cream sales in a 10 day period alongside the peak temperature that day.

| Day number | Peak temperature (°C)    | Number of sales |
|:-----------|:-------------------------|:----------------|
| 1          | 22                       | 150             |
| 2          | 20                       | 100             |
| 3          | 19                       | 110             |
| 4          | 21                       | 210             |
| 5          | 24                       | 260             |
| 6          | 25                       | 280             |
| 7          | 27                       | 310             |
| 8          | 26                       | 350             |
| 9          | 28                       | 360             |
| 10         | 25                       | 270             |

You can define $Y$ to be the random variable associated with observations $x$. Then, you can plot the graph of the simple linear regression model:

$$\mathbb{E}(Y) = \alpha + \beta x$$
:::

::: {.content-visible when-format="html"}

Use the tool below to observe how changing the values of **regression parameters** affects how closely the **simple linear regression model** fits the observed data.


```{shinylive-r}
#| standalone: true
#| viewerHeight: 600

library(shiny)
library(bslib)
library(ggplot2)
library(plotly)

# Ice cream sales data
ice_cream_data <- data.frame(
  day = 1:10,
  temperature = c(22, 20, 19, 21, 24, 25, 27, 26, 28, 25),
  sales = c(150, 100, 110, 210, 260, 280, 310, 350, 360, 270)
)

ui <- page_sidebar(
  title = "Ice Cream Sales Linear Model",
  sidebar = sidebar(
    numericInput("alpha", "Intercept (α)", value = -500, step = 10),
    numericInput("beta", "Slope (β)", value = 25, step = 1),
    hr()
  ),
  card(
    card_header("Ice Cream Sales vs Temperature"),
    plotlyOutput("regression_plot", height = "500px"),
    card_body(
      p("The scatter plot shows the relationship between temperature and ice cream sales. 
        The blue line represents your fitted linear model: E(Y) = α + βx.
        Adjust the model parameters (α and β) to find the line that best fits the data points.
        You can hover over the graph to see precise coordinates.")
    )
  )
)

server <- function(input, output, session) {
  output$regression_plot <- renderPlotly({
    # Create regression line values
    temp_range <- seq(18, 30, length.out = 100)
    predicted_sales <- input$alpha + input$beta * temp_range
    
    # Create the plot
    p <- ggplot() +
      # Add the observed data points
      geom_point(data = ice_cream_data, 
                aes(x = temperature, y = sales),
                size = 3, color = "#E31A1C", alpha = 0.8) +
      # Add the regression line
      geom_line(aes(x = temp_range, y = predicted_sales),
               color = "#007ACC", linewidth = 1.5) +
      labs(
        title = paste("E(Y) =", input$alpha, "+", input$beta, "× temperature"),
        x = "Average Temperature (°C)",
        y = "Number of Ice Cream Sales"
      ) +
      xlim(18, 30) +
      ylim(0, 400) +
      theme_minimal() +
      theme(
        plot.title = element_text(size = 14, hjust = 0.5),
        axis.title = element_text(size = 12)
      ) +
      # Add grid lines
      geom_hline(yintercept = seq(0, 400, 50), alpha = 0.2) +
      geom_vline(xintercept = seq(18, 30, 2), alpha = 0.2)
    
    # Convert to plotly for interactivity
    ggplotly(p, tooltip = c("x", "y")) %>%
      config(displayModeBar = FALSE)
  })
}

shinyApp(ui, server)
```

:::

::: {.content-hidden when-format="html"}

The graphs below illustrate how changing the values of the **regression parameters** affects how closely the **simple linear regression model** fits the observed data.

![A simple linear regression model for ice cream sales vs. temperature, with parameters $\alpha = -500$ and $\beta=25$ ](./FiguresPNG/linearregression-fig4.png)

![A simple linear regression model for ice cream sales vs. temperature, with parameters $\alpha = -480$ and $\beta=30$ ](./FiguresPNG/linearregression-fig5.png)

You can see that the line in Figure 2 is much closer to the data points than the line in Figure 1, meaning the equation in Figure 2 is a better fitted linear model.

:::

<!-- You have now seen that there is a connection between the values of the regression parameters and how well the regression line fits the data.  -->

:::{.callout-tip}
There are $n \in \mathbb{N}$ observations in any sample of data.

If you consider $i \in \mathbb{Z}$ such that $1 \leq i \leq n$, the $i^{th}$ data point on the graph of your data is given by $(x_{i},y_{i})$ and its corresponding estimated value is given by:
$$\mathbb{E}(Y_{i}) = \alpha + \beta x_{i}$$
:::

In practice, it is not always possible to plot the data points on a set of axes and manually tweak the values of the regression parameters until they appear to match the data - especially when you are working with a large sample of data. A quantitative method is a far more practical, so you will now learn how to use **residuals** in the **method of least squares estimation** to find the optimal regression line for your data.

## Least squares estimation

::: {.callout-note appearance="simple"}
## Definition of residuals

The **residual** describes the difference between the **observed** value $y_i$ and the **estimated** value $\mathbb{E}(Y_{i})$. 

Each **residual** is denoted $e_{i}$, where $i \in \mathbb{Z}$ such that $1 \leq i \leq n$. 

So, using the equation for $\mathbb{E}(Y_{i})$, you can define the residuals by:

$$e_i = y_i - E(Y_i) = y_i -  (\alpha + \beta x_{i})$$
:::

:::{.callout-tip}
When plotted on a graph, the residual can be interpreted as the vertical difference between the line of best fit and the plotted data point.
:::

You want $\mathbb{E}(Y_{i})$ to be as close to your observed data as closely as possible, so you want the difference between your **observed** data and the data **estimated** by the model to be as small as possible. To find the values of the regression parameters which achieve this, you can use the method of least squares estimation.

::: {.callout-note appearance="simple"}
## Least squares estimation

**Least squares estimation** is used to minimize the **sum of the squares of the residuals**, which is defined as $S(\alpha,\beta)$. 

You consider the squares of the residuals ($e_{i}^{2}$) to ensure that the summed values are always positive.

So, considering the sum of each $e_{i}^{2}$, you will find:
$$S(\alpha,\beta) = \sum_{i=1}^{n} e_{i}^{2} = \sum_{i=1}^{n}(y_{i} - (\alpha + \beta x_{i}))^{2}$$
The **method of least squares** minimizes this function to find **estimates** (sometimes known as **point estimates**) for the **regression parameters**. These are denoted by $\hat{\alpha}$ and $\hat{\beta}$, and these estimates generate a line of best fit where the average difference between the **observed** data and that **estimated** by the model is as small as possible.

:::

:::{.callout-important}
To find estimates for the regression parameters, you need to understand the following terminology:

$\bar{x}$ and $\bar{y}$ denote the **sample means** of $x$ and $y$.

The sum of the squared differences between each observation $x_{i}$ and the sample mean $\bar{x}$ is given by:
$$SS_{XX} = \sum_{i=1}^{n} (x_{i} - \bar{x})^2$$ 
The sum of the differences between each observation $x_{i}$ and the sample mean $\bar{x}$, multiplied by the differences between each observation $y_{i}$ and the sample mean $\bar{y}$ is described by:

$$SS_{XY} = \sum_{i=1}^{n} (x_{i} - \bar{x})(y_{i} - \bar{y})$$

For more detail about these terms, please read [Overview: Statistical notation](../overviews/o-notation.qmd).
:::

## Finding estimates of the regression parameters

You can differentiate $S(\alpha,\beta)$ with respect to the parameters $\alpha$ and $\beta$ and set these equal to $0$ to find the point at which there is no change in the value of the parameters. These are the estimates for the parameters. For more about derivatives, please read [Guide: Introduction to differentiation](introtodifferentiation.qmd).

The **least squares estimate** for the regression parameter $\beta$ is defined as:

$$\hat{\beta} = \frac {SS_{XY}} {SS_{XX}} = \frac{ \sum_{i=1}^{n} (x_{i} - \bar{x})(y_{i} - \bar{y})} { \sum_{i=1}^{n} (x_{i} - \bar{x})^{2}}$$

The **least squares estimate** for the regression parameter $\alpha$ is defined as:

$$\hat{\alpha} = \bar{y} - \hat{\beta}\bar{x}$$

For proofs of these results, please read [Proof sheet: Deriving estimates for regression parameters].

:::{.callout-tip}
When working with sums, you use programming languages such as R to save time manually typing each value into a calculator!
:::

::: {.content-visible when-format="html"}

::: {.callout-note appearance="simple"}

## Example 2

Use the tool below to explore how changing the values of the **regression parameters** affects how close the estimated values $\mathbb{E}(Y_i)$ are to the observed values $y_i$. You can see that the residuals are the vertical differences between the observed and expected values. 

:::

```{shinylive-r}
#| standalone: true
#| viewerHeight: 800

library(shiny)
library(ggplot2)
library(bslib)
library(plotly)

ui <- page_sidebar(
  title = "Least Squares Estimation Explorer",
  sidebar = sidebar(
    sliderInput("intercept", "Intercept (α)", 
                min = -5, max = 10, value = 2, step = 0.1),
    sliderInput("slope", "Slope (β)", 
                min = -2, max = 4, value = 1.5, step = 0.1),
    checkboxInput("show_residuals", "Show residuals", value = TRUE),
    hr(),
    card(
      card_header(
        h5("Least Squares Estimates", style = "margin: 0;")
      ),
      card_body(
        div(
          style = "font-family: 'Courier New', monospace; font-size: 16px; font-weight: bold; text-align: center; padding: 10px;",
          uiOutput("dataInfo")
        )
      )
    )
  ),
  # Stack cards vertically using layout_columns with col_widths = 12
  layout_columns(
    col_widths = 12,
    card(
      card_header("Linear Model Plot"),
      plotlyOutput("regPlot", height = "500px")
    ),
    card(
      card_header("Key"),
      div(
        style = "font-size: 0.9em; line-height: 1.4;",
        tags$div(tags$span("●", style = "color: black; font-size: 1.2em;"), " Data Points"),
        tags$div(tags$span("- - -", style = "color: blue; font-weight: bold;"), " Your Guess"),
        tags$div(tags$span("——", style = "color: red; font-weight: bold;"), " Least Squares Solution"),
        conditionalPanel(
          condition = "input.show_residuals == true",
          tags$div(tags$span("⋯⋯⋯", style = "color: blue;"), " Residuals")
        )
      )
    )
  )
)

server <- function(input, output, session) {
  
  # Generate simulated data
  values <- reactiveValues()
  
  # Initialize data
  observe({
    if (is.null(values$data)) {
      set.seed(123)
      x <- runif(30, 0, 10)
      y <- 1.5 * x + 2 + rnorm(30, 0, 2)
      values$data <- data.frame(x = x, y = y)
      values$lm_model <- lm(y ~ x, data = values$data)
    }
  })
  
  # Data information with better formatting using HTML
  output$dataInfo <- renderUI({
    req(values$lm_model)
    ls_coef <- coef(values$lm_model)
    
    div(
      div(
        HTML(paste0("α̂ = ", span(round(ls_coef[1], 3), style = "color: #0066cc;"))),
        style = "margin-bottom: 8px;"
      ),
      div(
        HTML(paste0("β̂ = ", span(round(ls_coef[2], 3), style = "color: #0066cc;")))
      )
    )
  })
  
  # Interactive plot with hover functionality
  output$regPlot <- renderPlotly({
    req(values$data, values$lm_model)
    d <- values$data
    
    # Calculate predictions
    ls_pred <- predict(values$lm_model)
    guess_pred <- input$slope * d$x + input$intercept
    
    # Create base plot
    p <- ggplot(d, aes(x, y)) +
      geom_point(size = 3, color = "black", alpha = 0.7,
                 aes(text = paste("x:", round(x, 2), "<br>y:", round(y, 2)))) +
      geom_abline(slope = input$slope, intercept = input$intercept,
                  color = "blue", linetype = "dashed", linewidth = 1.5) +
      geom_line(aes(y = ls_pred), color = "red", linewidth = 1.5) +
      labs(
        title = paste("Your Model: E(Y) =", round(input$intercept, 2), "+", 
                     paste0(round(input$slope, 2), "x")),
        x = "x",
        y = "y"
      ) +
      theme_minimal(base_size = 12) +
      xlim(0, 10) +
      ylim(min(d$y) - 2, max(d$y) + 2)
    
    # Add residuals if requested
    if (input$show_residuals) {
      p <- p + geom_segment(aes(xend = x, yend = guess_pred),
                            color = "blue", linetype = "dotted", alpha = 0.7)
    }
    
    # Convert to plotly with hover tooltips
    ggplotly(p, tooltip = "text") %>%
      config(displayModeBar = FALSE)
  })
}

shinyApp(ui = ui, server = server)


```

:::


::: {.content-hidden when-format="html"}

::: {.callout-note appearance="simple"}

## Example 2

The graphs below demonstrate how changing the values of the **regression parameters** affects how close the estimated values $\mathbb{E}(Y_i)$ are to the observed values $y_i$. You can see that the residuals are the vertical differences between the observed and expected values. 

:::


![A regression line with parameters $\alpha = 2$ and $\beta=1.5$ plotted alongside the regression line with parameters $\hat{\alpha}$ and $\hat{\beta}$.](./FiguresPNG/linearregression-fig6.png)

:::

You can see that the regression line which uses the least squares estimates fits the data the closest.

:::{.callout-tip}
When working with sums, you use programming languages such as R to save time manually typing each value into a calculator.
:::

::: {.callout-note appearance="simple"}
## Example 3

In Example 1, you saw the following data from Cantor's Confectionery.

| Day number | Peak temperature (°C)    | Number of sales |
|:-----------|:-------------------------|:----------------|
| 1          | 22                       | 150             |
| 2          | 20                       | 100             |
| 3          | 19                       | 110             |
| 4          | 21                       | 210             |
| 5          | 24                       | 260             |
| 6          | 25                       | 280             |
| 7          | 27                       | 310             |
| 8          | 26                       | 350             |
| 9          | 28                       | 360             |
| 10         | 25                       | 270             |

You defined temperature as $x$ and the number of ice cream sales as $y$.

You saw how changing the values of the **regression parameters** $\alpha$ and $\beta$ affected how closely the **simple linear regression model** $\mathbb{E}(Y) = \alpha + \beta x$  fitted the observed data.

Now, you will use **least squares estimation** to estimate the values $\hat{\alpha}$ and $\hat{\beta}$ which best fit the data.

You can see that there are $n = 10$ entries in the table. You can use the definition of sample means - as seen in [Overview: Statistical notation](../overviews/o-notation.qmd) - to obtain values of $\bar{x}$ and $\bar{y}$.

$$\bar{x} = \frac{1}{n} {\sum_{i=1}^{n} x_{i}} = \frac{1}{10} {\sum_{i=1}^{n} x_{i}} = 23.7$$

$$\bar{y} = \frac{1}{n} {\sum_{i=1}^{n} y_{i}} = \frac{1}{10} {\sum_{i=1}^{n} y_{i}} = 240$$
Use these sample means to calculate $SS_{XX}$ and $SS_{XY}$:

$$SS_{XY} = \sum_{i=1}^{n} (x_{i} - \bar{x})(y_{i} - \bar{y}) = 2460$$

$$SS_{XX} = \sum_{i=1}^{n} (x_{i} - \bar{x})^{2} = 84.1$$
Using these results, you can find estimates for the regression parameters.

$$\hat{\beta} = \frac {SS_{XY}} {SS_{XX}} = \frac {2460} {84.1} = 29.25089$$

$$\hat{\alpha} = \bar{y} - \hat{\beta}\bar{x} = 240 - 29.25 \cdot 23.7 = -453.2461$$

:::

As you saw earlier, the observations $Y_{1}, ..., Y_{n}$ are random variables and can therefore be assumed to follow a distribution. Since the observations are functions of the **regression parameters**, the least squares estimates for the regression parameters are also random variables.

For more information about this, please read [Guide: Maximum likelihood estimation].

Therefore, you can assume that the estimates follow a distribution. 

For more information about the types of probability distributions which your data could follow, please read [Overview: Probability distributions](../overviews/o-distributions.qmd).

:::{.callout-important}

Before you can assume that your data follows a particular distribution, you must check whether your data aligns with the assumptions which underpin this distribution.

:::

It is useful for your regression parameters to follow probability distributions because it allows you to construct confidence intervals and test hypotheses about them.

Constructing confidence intervals for the regression parameters enables you to construct a plausible range of values, within which it is likely that your estimation lies. This recognizes the uncertainty as to how closely your sample matches the real-life scenario. 

For more information about confidence intervals, please read [Guide: Confidence intervals](confidenceintervals.qmd).

To learn how to apply these ideas to data which follows a **normal distribution**, please read [Guide: Normal linear regression.]

Testing hypotheses is particularly useful when there are multiple explanatory variables, because you can test whether some of them are equal to zero - and if so, they can be discounted. This reduces the complexity of the linear regression model, meaning it is more transferable to new data. 

For more on this, please read [Guide: Introduction to Multiple Linear Regression].



<!-- # Normal linear regression -->

<!-- Suppose that you have a set of n observations. -->

<!-- You previously saw that for $i \in \mathbb{Z}$ such that $1 \leq i \leq n$,  -->
<!-- $$\mathbb{E}(Y_i) = \alpha + \beta x_{i}$$ -->

<!-- As seen in [Overview: Statistical notation](../overviews/o-notation.qmd), this is equal to the sample mean. -->

<!-- Now suppose that the variables $Y_{1}, ..., Y_{n}$ have variance $\sigma^{2}$. -->

<!-- Then, the distribution of the $i^{th}$ random variable $Y_{i}$ can be expressed as: -->
<!-- $$Y _{i} \sim N (\alpha + \beta x_{i} , \sigma^{2})$$ -->


<!-- ::: {.callout-note appearance="simple"} -->

<!-- ## Assumptions for the normal linear regression model -->

<!-- You must check that your data can be modelled using the normal distribution before you can use this model. This means that you must check that your data satisfies the following conditions: -->

<!-- a) **Independence** - $Y_{1}, ..., Y_{n}$ are independent random variables. -->

<!-- b) **Normality** - $Y_{1}, ..., Y_{n}$ are normally distributed.  -->

<!-- c) **Linearity** - the mean of $Y_{i}$ is a linear function of $x_{i}$, or equivalently, $\mathbb{E}(Y) = \alpha + \beta x_{i}$. -->

<!-- d) **Constant variance** - $Y_{1}, ..., Y_{n}$ have the same variance. -->

<!-- ::: -->

<!-- :::{.callout-tip} -->

<!-- You can use the programming language **R** to produce plots which allow you to check whether your data aligns with assumptions (b)-(d). Although details of how to do this in R are omitted from this guide, the subsequent example will demonstrate the way to interpret such graphs. -->

<!-- You assess the general trend of the plotted data points on the graphs, rather than examining specific values. -->

<!-- ::: -->

<!-- ::: {.callout-note appearance="simple"} -->

<!-- ## Example 3 -->

<!-- Cantor's Confectionery now wants to predict the number of ice cream sales each day $y_{i}$ based on the peak daily temperature $x_{i}$ using a **normal** linear regression model. Based on the graphs below, does the assumption of normality seem acceptable? -->

<!-- ::: -->

<!-- You must check that all four conditions (independence, normality, linearity and constant variance) are true for your data. -->

<!-- a) **Independence** -->

<!-- Since each data point comes from a different day, the assumption of independence is reasonable. -->

<!-- b) **Normality** -->

<!-- If the data was distributed normally, the points would lie along the dotted line of the Q-Q plot. -->

<!-- ![The Q-Q plot for the the given data.](./FiguresPNG/linearregression-fig1.png) -->

<!-- Here, you can see that the points stray from the dashed line, especially at the ends, which suggests that the ice cream sales may not be normally distributed. -->

<!-- c) **Linearity** -->

<!-- If the data was linear, the red line of the residuals vs. fitted plot for the given data would be mostly horizontal. -->

<!-- ![The residuals vs. fitted plot for the the given data.](./FiguresPNG/linearregression-fig2.png) -->

<!-- Here, you can see that the red line is mostly horizontal, which indicates that the data is linear. -->

<!-- d) **Constant variance** -->

<!-- The assumption holds if there is not much change in the vertical spread of data on a **residuals vs. fitted plot** and if the red line is mostly horizontal on a **scale-location plot**. -->

<!-- ![The residuals vs. fitted plot for the the given data.](./FiguresPNG/linearregression-fig2.png) -->

<!-- You can see that the vertical spread of the points seems to vary, but because there aren't many points, you should look at the scale-location plot for further insight. -->

<!-- ![The scale-location plot for the the given data.](./FiguresPNG/linearregression-fig3.png) -->

<!-- You can see that the red line is quite curved, which indicates that the variance is not constant. -->

<!-- Since each random variable does not appear to be normally distributed and the data, nor do they appear to have the same variance, the normal linear regression model does not seem to be suitable for this set of data. However, more samples are needed to form a more substantiated conclusion. -->

<!-- However, if the assumptions do not hold, you could apply a transformation to your data - for more details, please read [Guide: Transforming data in linear regression]. -->

<!-- ## Fitting the normal linear regression model -->

<!-- As you saw earlier, to fit the **linear model** to your data, you need to estimate the values of the **regression parameters**.  -->

<!-- ::: {.callout-note appearance="simple"} -->

<!-- ## Distributions of the regression parameters -->

<!-- When your data is normally distributed, the regression parameters are also normally distributed, so you use the **likelihood function** on random variable $Y_i$ to obtain values for the variances of $(\hat{\alpha}$ and $\hat{\beta} x_{i})$.  -->

<!-- For more on likelihood functions, please read [Guide: Likelihood functions]. -->

<!-- These are given by: -->
<!-- $$\operatorname{Var}(\hat{\alpha}) = \sigma^{2}\!\left(\frac{1}{n} + \frac{\bar{x}^{2}}{SS_{XX}}\right)$$ -->
<!-- and  -->
<!-- $$\operatorname{Var}(\hat{\beta}) = \frac{\sigma^{2}}{SS_{XX}}$$ -->
<!-- For proofs of these results, please go to [Proof: variances of regression parameters in normal linear regression]. -->

<!-- As seen earlier, $(\hat{\alpha}$ and $\hat{\beta}$ are unbiased estimators of $\alpha$ and $\beta$, so the mean of each is the true value of the parameter. -->

<!-- For more information about this, please read [Guide: Expected value, variance and standard deviation](expectedvariance.qmd). -->

<!-- So, -->

<!-- $$\hat{\alpha} \sim N\left( \alpha, \sigma^{2}\left( \frac{1}{n} + \frac{\bar{x}^{2}}{SS_{XX}} \right) \right)$$ -->

<!-- $$\hat{\beta} \sim N\left( \beta,\; \frac{\sigma^{2}}{SS_{XX}} \right)$$ -->
<!-- ::: -->

<!-- Because you know the distributions of the regression parameters, you can construct confidence intervals for the values of the regression parameters.  -->


<!-- ## Finding confidence intervals of regression parameters -->

<!-- :::{.callout-warning} -->
<!-- In statistics, $\alpha$ is often used when defining confidence intervals. This is **different** to the regression parameter $\alpha$. -->
<!-- ::: -->

<!-- Typically, statisticians attempt to find $1-\alpha\%$ confidence intervals for the regression parameters $\alpha$ and $\beta$. -->

<!-- :::{.callout-important} -->
<!-- To find estimates for the regression parameters, you need to understand the following terminology: -->

<!-- The estimator for $y_{i}$ is given by: -->
<!-- $$\hat{y}_i = \hat{\alpha} + \hat{\beta} x_{i}$$ -->

<!-- Recalling that each $Y_i$ is a random variable, the sample variance is given by: -->
<!-- $$S^{2} = \frac{1}{n - 2} \sum_{i=1}^{n} (Y_{i} - \hat{y}_{i})^{2}$$ -->

<!-- For more about sample variance, please read [Overview: Statistical notation](../overviews/o-notation.qmd). -->


<!-- $t_{n-2; 1 - \frac{\alpha}{2}}$ describes the value $w$ of the $t$-distribution with $n-2$ degrees of freedom where the probability $P(T \leq w) = 1 - \frac{\alpha}{2}$. -->

<!-- This is illustrated in the following graph: -->

<!-- ![The graph of the t-distribution with $n=8$ degrees of freedom, showing the value of w which is needed to find the correct confidence interval.](./FiguresPNG/linearregression-fig7.png) -->

<!-- For more on the $t$-distribution, including a calculator to find $t$-values, please go to [Fact sheet: $t$-distribution](../factsheets/f-tdist.qmd). -->
<!-- ::: -->

<!-- ::: {.callout-note appearance="simple"} -->
<!-- ## Formulae for finding confidence intervals of normally distributed regression parameters -->
<!-- The $1-\alpha$% confidence interval for $\alpha$ is given by: -->
<!-- $$\hat{\alpha} \pm t_{n-2; 1 - \frac{\alpha}{2}} \sqrt{S^2 \left (\frac{1}{n} + \frac{\bar{x}^2}{SS_{XX}} \right)}$$ -->

<!-- The $1-\alpha$% confidence interval for $\beta$ is given by: -->
<!-- $$\hat{\beta} \pm t_{n-2; 1 - \frac{\alpha}{2}} \sqrt{\frac{S^2}{SS_{XX}}}$$ -->

<!-- For proofs of these results, please read [Proof sheet: Estimating regression parameters in normal linear regression]. -->

<!-- ::: -->


<!-- ::: {.callout-note appearance="simple"} -->

<!-- ## Example 4 -->

<!-- Despite the statistician's conclusion about whether the normal linear regression is appropriate (as seen in example 3), Cantor's Confectionery would like you to assume that the normal regression model is appropriate, and to use it to find $95$% confidence intervals for the regression parameters $\alpha$ and $\beta$.  -->

<!-- In Example 2, you saw that $n = 10$, $\bar{x} = 23.7$, $\hat{\alpha} = -453.2461$, $\hat{\beta} = 29.25089$, $SS_{XY} = 2460$ and $SS_{XX} = 84.1$ -->

<!-- So the sample variance is given by: -->

<!-- $$S^2 = \frac{1}{10 - 2} \sum_{i=1}^{n} (Y_{i} - \hat{y}_{i})^{2} = 1241.24$$ -->

<!-- <!-- By rearranging the equation $0.95 = 1 - \frac{\alpha}{2}$, you can see that $\alpha = 0.025$. --> 
<!-- By using a $t$-distribution calculator, you can find the $t$-value: -->

<!-- $$t_{10-2; 1 - \frac{\alpha}{2}} = t_{8;0.975} = 2.306$$ -->

<!-- Therefore, the $1-\alpha$% confidence interval for $\alpha$ is given by: -->

<!-- $$\begin{aligned}\hat{\alpha} \pm t_{10-2; 1 - \frac{\alpha}{2}} \sqrt{S^2 \left (\frac{1}{n} + \frac{\bar{x}^2}{SS_{XX}} \right)} &= -453.2461 \pm 2.306 \sqrt{1241.24 \left (\frac{1}{10} + \frac{23.7^2}{84.1} \right)}\\ &= (-664.8, -241.7) \end{aligned}$$ -->

<!-- And the $1-\alpha$% confidence interval for $\beta$ is given by: -->
<!-- $$\begin{aligned} \hat{\beta} \pm t_{10-2;\, 1 - \frac{\alpha}{2}} \sqrt{\frac{S^{2}}{SS_{XX}}} &= 29.25089 \pm 2.306 \sqrt{\frac{1241.24}{84.1}} \\ &= (20.39, 38.11)\end{aligned}$$ -->

<!-- ::: -->


<!-- ::: {.callout-note appearance="simple"} -->

<!-- ## Likelihood function -->

<!-- Given the independently and identically distributed variables $Y_{1}, ..., Y_{n}$, where $Y_{i} \sim N(\alpha + \beta x_{i}, \sigma^{2})$ for $i \in [1,n]$, you can write down the likelihood function of $\alpha$ and $\beta$. -->

<!-- To estimate the values of $\alpha$ and $\beta$, you can use the log-likelihood function: -->
<!-- $$l(\alpha, \beta, \sigma^2; (x_1, y_1), \ldots, (x_n, y_n)) = -\frac{n}{2} \log(2\pi\sigma^2)  -->
<!-- \ - \ \frac{1}{2\sigma^2} \sum_{i=1}^{n} \left( y_i - (\alpha + \beta x_i) \right)^2$$ -->

<!-- For more on likelihood functions, please go to [Guide: Likelihood functions].  -->

<!-- For a derivation of this equation, please go to [Proof sheet: Estimating Regression Parameters in Normal Linear Regression]. -->

<!-- ::: -->

<!-- :::{.callout-tip} -->

<!-- The log-likelihood can be maximized with respect to $\alpha$ and $\beta$ when $\sigma^{2}$ is unknown.  -->

<!-- If you maximize the log-likelihood function with respect to the regression parameters $\alpha$ and $\beta$, this is equivalent to minimizing $S(\alpha,\beta)$. -->

<!-- ::: -->

<!-- Another extension to linear regression is when there are multiple explanatory variables affecting the response variable. This is called **multiple linear regression** and for more detail on this, please read [Guide: Multiple linear regression.] -->

<!-- # Multiple linear regression -->

<!-- So far, there has only been one explanatory variable affecting the response variable. Unfortunately, events in the real world are usually affected by multiple factors! Thankfully, the simple linear regression model which has been discussed so far can be extended to include multiple explanatory variables. -->

<!-- ::: {.callout-note appearance="simple"} -->
<!-- ## The multiple linear regression model -->

<!-- The **multiple linear regression model** is a generalization of the simple linear regression model. The response variable $Y$ is related to the explanatory variables $x_{1}, x_{2}, ... x_{n}$ by: -->

<!-- $$\mathbb{E}(Y_{i}) = \alpha + \sum_{k=1}^{n} \beta_{k} x_{ki} = \alpha + \beta_{1} x_{1i} + ... + \beta_{k} x_{ki}$$ -->

<!-- It is assumed that each random variable $Y_{i}$ follows the same distribution. -->

<!-- The least-squares estimates $\hat{\alpha}, \hat{\beta_1}, \dots, \hat{\beta_k}$ of $\alpha, \beta_1, \dots, \beta_k$ are defined as the values which minimize the function -->

<!-- $$S(\alpha, \beta_1, \dots, \beta_k) = \sum_{i=1}^{n} \left( y_i - \left( \alpha + \beta_1 x_{i1} + \dots + \beta_k x_{ik} \right) \right)^2$$ -->

<!-- where $y_i$ is the response at the value $(x_{i1}, \dots, x_{ik})$ of the explanatory variables $(x_1, \dots, x_k)$. -->

<!-- So, when considering the normal distribution, the linear regression model becomes: -->

<!-- $$Y_{i} \sim N (\alpha + \sum_{m=1}^{k} \beta_{m} x_{mi}, \sigma^{2})$$ -->
<!-- where $Y_{1},...Y_{n}$ are independent. -->
<!-- ::: -->

# Quick check problems

<!-- add facility for webexercises to work on html -->

:::: {.content-visible when-format="html" data-topic="LR1"}
<!-- add facility to check answers at end rather than one at a time -->

::: {.webex-check .webex-box}
1.  Which of these is the equation of a linear regression model?

(a) $\mathbb{E}(Y) = \alpha + \beta x_{i}$

(b) $\hat{\alpha} = \bar{y} - \hat{\beta}\bar{x}$

(c) $\hat{\beta} = \frac {SS_{XY}} {SS_{XX}}$

Answer: `r mcq(c(answer="a", "b", "c"))`.

2. What is the name of the type of linear regression with one explanatory variable?

Answer:`r mcq(c("Independent", answer = "Simple", "Single"))`.

3.  Are the following statements true or false?

(a) $e_i$ is used to describe a residual. Answer: `r torf(TRUE)`.

(b) To use linear regression, your data must be normally distributed. Answer: `r torf(FALSE)`.

(c) The regression parameter $\alpha$ can be equal to zero. Answer: `r torf(TRUE)`.
:::
::::

::: {.content-hidden when-format="html"}
1.  Which of these is the equation of a linear regression model?

<!-- -->
(a) $\mathbb{E}(Y) = \alpha + \beta x_{i}$

(b) $\hat{\alpha} = \bar{y} - \hat{\beta}\bar{x}$

(c) $\hat{\beta} = \frac {SS_{XY}} {SS_{XX}}$

2.  What is the name of the linear regression model which has one explanatory variable?

3.  Are the following statements true or false?

<!-- -->
(a) $e_i$ is used to describe a residual.

(b) To use linear regression, your data must be normally distributed.

(c) The regression parameter $\alpha$ can be equal to zero.
:::



# Further reading {-}

For more questions on the subject, please go to [Questions: Introduction to linear regression](../questions/qs-linearregression.qmd).

To learn about normal linear regression, please read [Guide: Normal linear regression].

To extend linear regression to encompass multiple explanatory variables, please read [Guide: Multiple linear regression].

## Version history {-}

v1.0: initial version created 12/25 by Flora Green as part of a University of St Andrews VIP project.
  
[This work is licensed under CC BY-NC-SA 4.0.](https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1)

