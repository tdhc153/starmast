{
  "hash": "a17ccfcf4c0cbadcdb8bf5c27055303c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Expected value, variance, standard deviation\nauthor: Tom Coleman\nabstract-title: Summary\nabstract: This guide introduces expected values, variance, and standard deviation. These are key ideas in the concept of random variables, and they give information about the distribution of data conforming to a random variable.\ncategories:\n  - Probability\n  - Statistics\nfilters:\n  - shinylive\nimage: FiguresPNG/expectedvariance-image.png\n---\n\n\n\n*Before reading this guide, it is highly recommended that you read [Guide: PMFs, PDFs, and CDFs](pmfspdfscdfs.qmd) and [Guide: Introduction to data analysis].*\n\n::: {.content-visible when-format=\"html\"}\n\n```{=html}\n<table><tr><td style=\"vertical-align: middle\"><strong>Narration of study guide:</strong>&nbsp;&nbsp;</td><td><audio controls><source src=\"./Narrations/expectedvariance.mp3\" type=\"audio/mpeg\">Your browser does not support the audio element.</audio></tr></table>\n```\n\n:::\n\n# Introduction {.unnumbered}\n\nIn [Guide: PMFs, PDFs, and CDFs](pmfspdfscdfs.qmd), you saw how these concepts are key tools in the study of probability, used to model and analyze the behaviour of random variables. As you have seen, these functions describe how probabilities are distributed across the possible outcomes of random events. For instance, the shape and position of the normal distribution (see Example 5 of [Guide: PMFs, PDFs, and CDFs](pmfspdfscdfs.qmd), or [Factsheet: The normal distribution](../factsheets/f-normaldist.qmd)) can be modified by specifying the **mean** $\\mu$ and the **standard deviation** $\\sigma$. But what are these quantities? What do they mean, and how can they be computed from a given PMF or PDF?\n\nThis guide illustrates the related concepts of the **expected value**, **variance**, and **standard deviation** of a random variable $X$, and explains their usage and properties in probability theory. These concepts are not only found in statistics alone, but also in fields like classical and quantum mechanics, machine learning, and in decision theory.\n\n<!-- # Mean -->\n\n<!-- The idea of a mean is central to data analysis, allowing you to find the central point of a set of data. -->\n\n<!-- ::: {.callout-note icon=\"true\"} -->\n<!-- ## Definition of the mean -->\n\n<!-- Suppose that you have a set of data $S = \\{x_1,x_2,\\ldots,x_n\\}$. The **mean** of the data is given by $$\\bar{x} = \\dfrac{x_1 + x_2 + \\ldots + x_n}{n} = \\dfrac{\\sum_{i = 1}^n x_i}{n}$$ that is, the sum of all of the elements of the set $S$ divided by the number of elements in $S$. -->\n<!-- ::: -->\n\n<!-- ::: callout-tip -->\n<!-- The symbol $\\sum$ is called **sigma notation** and represents the sum of all values in a particular set. In this example, it is adding the probabilities from all possible outcomes of a random variable $X$. For more examples, see [Guide: Introduction to sigma notation.](sigmanotation.qmd) -->\n<!-- ::: -->\n\n<!-- This is the **average** of the given data set. Here's an example of the mean in action. -->\n\n<!-- ::: {.callout-note appearance=\"simple\"} -->\n<!-- ## Example 1 -->\n\n<!-- The mean of the data set $\\{1,2,3,4,5,6\\}$ is $$\\bar{x} = \\dfrac{1+2+3+4+5+6}{6} = \\frac{21}{6} = 3.5.$$ This shows that the mean does not have to be a particular value in the data set (unlike the mode, for instance). -->\n<!-- ::: -->\n\n<!-- ::: {.callout-note appearance=\"simple\"} -->\n<!-- ## Example 2 -->\n\n<!-- You select five random Boole Bars from Cantor's confectionery and measure their lengths, which gives a data set of $\\{4.53, 4.61, 4.33, 4.51, 4.52\\}$ (all lengths measured in centimetres). The average length of a Boole bar from this data set is $$\\bar{x} = \\dfrac{4.53 + 4.61 + 4.33 + 4.51 + 4.52}{5} = \\frac{22.5}{5} = 4.5.$$ -->\n<!-- ::: -->\n\n<!-- ## Sample mean versus population mean {.unnumbered} -->\n\n<!-- You might have noticed here that the mean in Examples 1 and 2 was written as $\\bar{x}$, whereas the mean in the statement of the normal distribution is written as $\\mu$ (the Greek letter mu). The difference here is the idea of a **sample mean** $\\bar{x}$ versus the **population mean** $\\mu$. The two might be very different depending on the sample! -->\n\n<!-- For instance, in Example 2, you worked out the sample mean length for a chocolate bar, but this doesn't necessarily mean that this is the mean of all of the chocolate bars ever made! This is an example of the difference between the sample mean and the population mean. -->\n\n<!-- Often, population means can be very difficult to find. Suppose you wanted to find the average height of every adult in your home country. You would then have to potentially ask millions of people to report their height, and then work out the average. This is extremely impractical, and so a sample mean must be taken instead. To measure if your sample mean is a statistically significant representative of the population mean, you could use a **hypothesis test**; see [Guide: Hypothesis testing](hypothesistesting.qmd) for more. -->\n\n<!-- Typically, what you would do is **model** the population to a certain probability distribution, and use the **expected value** of that probability distribution as an estimate of the population mean. This allows you to extrapolate statements about your sample to fit the entire population. -->\n\n# Expected value {.unnumbered}\n\nOften, the arithmetic mean of the outcomes is not the correct way to go when it comes to thinking about where the 'middle' of the set of data (conforming to a random variable). This is because some outcomes are more likely to occur than others. More formally, in a PMF or PDF describing a probability distribution, it is not guaranteed that every value is equally likely to occur.\n\nFor instance, suppose you have two six-sided dice; one is fair, and one is biased to give a lower value more often than not.\n\n| $x$ | 1 | 2 | 3 | 4 | 5 | 6 |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| $\\mathbb{P}(X = x)$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ |\n\nTable 1: PMF for rolling a fair six-sided die.\n\n| $y$ | 1 | 2 | 3 | 4 | 5 | 6 |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| $\\mathbb{P}(Y = y)$ | $\\dfrac{1}{4}$ | $\\dfrac{1}{4}$ | $\\dfrac{1}{4}$ | $\\dfrac{1}{12}$ | $\\dfrac{1}{12}$ | $\\dfrac{1}{12}$ |\n\nTable 2: PMF for rolling a biased six-sided die.\n\nYou could ask; if you rolled one of these dice, what is the value that you would expect to see? With the fair, it would be between $3$ and $4$ as every outcome is equally likely to occur. With the biased die, you are more likely to get a smaller number as the probability of getting a number less than or equal to three is higher. This is because the PMFs for these dice are different.\n\nSo how can you measure the 'expected value' of a random variable given by some PMF or PDF? The answer is to 'weight' the outcomes by their probabilities; this is the definition of an **expected value**:\n\n::: {.callout-note icon=\"true\"}\n## Definition of the expected value\n\nSuppose that you have a random variable $X$.\n\n-   If $X$ is a discrete random variable with probability mass function $p(x)$, then the **expected value** $\\mathbb{E}(X)$ of $X$ is given by $$\\mathbb{E}(X) = \\sum_{x} xp(x) = \\sum_{x} x\\mathbb{P}(X=x)$$ that is, the sum of an outcome $x$ multiplied by its probability $p(x) = \\mathbb{P}(X=x)$ over all possible outcomes $x$.\n\n-   If $X$ is a continuous random variable with probability density function $f(x)$, then the **expected value** $\\mathbb{E}(X)$ of $X$ is given by $$\\mathbb{E}(X) = \\int_{-\\infty}^\\infty xf(x) \\,\\textrm{d}x$$ that is, the integral of $x$ multiplied by its probability $f(x)$ over all possible values of $x$ in the real numbers.\n:::\n\n::: callout-tip\nGeometrically, the expected value is the 'central point' of the distribution of the data. See below for more details on this.\n:::\n\n::: {.callout-note appearance=\"simple\"}\n## Example 1\n\nLet's look at the expected values of the two dice given above. The PMF for the fair die is given by\n\n| $x$ | 1 | 2 | 3 | 4 | 5 | 6 |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| $\\mathbb{P}(X = x)$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ | $\\dfrac{1}{6}$ |\n\nso the expected value $\\mathbb{E}(X)$ of this die is $$\\begin{aligned}\\mathbb{E}(X) &= 1\\cdot \\frac{1}{6} + 2\\cdot \\frac{1}{6} + 3\\cdot \\frac{1}{6} + 4\\cdot \\frac{1}{6} + 5\\cdot \\frac{1}{6} + 6\\cdot \\frac{1}{6}\\\\[1em] &= \\dfrac{1 + 2 + 3 + 4 + 5 + 6}{6} = 3.5\\end{aligned}$$ You can notice that this is the same as the mean of the outcomes; more on this later.\n\nWhat about for the second, biased die? This has PMF\n\n| $y$ | 1 | 2 | 3 | 4 | 5 | 6 |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| $\\mathbb{P}(Y = y)$ | $\\dfrac{1}{4}$ | $\\dfrac{1}{4}$ | $\\dfrac{1}{4}$ | $\\dfrac{1}{12}$ | $\\dfrac{1}{12}$ | $\\dfrac{1}{12}$ |\n\nso the expected value $\\mathbb{E}(Y)$ of this die is $$\\begin{aligned}\\mathbb{E}(Y) &= 1\\cdot \\frac{1}{4} + 2\\cdot \\frac{1}{4} + 3\\cdot \\frac{1}{4} + 4\\cdot \\frac{1}{12} + 5\\cdot \\frac{1}{12} + 6\\cdot \\frac{1}{12}\\\\[1em] &= \\dfrac{1 + 2 + 3}{4} + \\dfrac{4 + 5 + 6}{12} = 2.75\\end{aligned}$$ which is **not** the mean of the outcomes.\n\nBoth of these 'expected values' are impossible to obtain in practice; try finding a die that shows $3.5$ spots on the top! This is because the expected value actually represents the 'central point' of the distribution, rather than a specific outcome.\n:::\n\n::: callout-tip\nFor a discrete random variable $X$ with finitely many values, the expected value $\\mathbb{E}(X)$ only equals the mean of the outcomes if and only if the probability of each outcome $\\mathbb{P}(X=x)$ is the same for every $x$. If this happens, then $\\mathbb{P}(X=x) = 1/n$, and so the definition of expected value becomes $$\\mathbb{E}(X) = \\sum_{x} x\\mathbb{P}(X=x) = \\sum_{x} x\\cdot \\frac{1}{n} = \\dfrac{\\sum_{x} x}{n}$$ which you can recognize as the mean.\n\nBut in all cases, the expected value of a random variable really is the 'population mean' of the probability distribution, which is not the same as the mean value of the outcomes. So **in theoretical probability distributions, the concept of the population mean of a random variable and the expected value of a random variable are the same.**\n:::\n\nHere's an example of the expected value that introduces the idea of a **Bernoulli trial**.\n\n::: {.callout-note appearance=\"simple\"}\n## Example 2\n\nImagine that you are in charge of an experiment that either succeeds with probability $p$ or fails with probability $q = 1-p$. You can write the outcome of success as $1$ and the outcome of failure as $0$. Therefore, the sample space is $\\{0,1\\}$ with PMF $b(0) = q = 1-p$ and $b(1) = p$. This is known as a **Bernoulli trial**.\n\nSuppose that $X$ is a random variable represented by a Bernoulli trial. What is the expected value $\\mathbb{E}(X)$? You can use the definition of expected value to work this out: $$\\mathbb{E}(X) = 0\\cdot (1-p) + 1\\cdot p = p$$ and so the expected value of a Bernoulli trial is the probability of success $p$.\n\nOf course, much like the expected value of the dice above, this is impossible to obtain in practice!\n:::\n\nMuch like many other concepts in mathematics and statistics, you can use properties of expected values to build upon knowledge to find expected values of other random variables.\n\n::: {.callout-note icon=\"true\"}\n## Properties of expected values\n\nLet $X$ be a random variable (either discrete or continuous). Then the following properties are true for expected values:\n\n1.  If $g$ is a function, then you can define a new random variable $Z = g(X)$ with the outcomes as $z = g(x)$.\n\n    -   If $X$ is discrete with PMF $f_X(x)$, then so is $Z$ and $$\\mathbb{E}(Z) = \\mathbb{E}(g(X)) = \\sum_{x} g(x)f_X(x).$$\n\n    -   If $X$ is continuous with PDF $f_X(x)$, then so is $Z$ and $$\\mathbb{E}(Z) = \\mathbb{E}(g(X)) = \\int_{-\\infty}^\\infty g(x)f_X(x)\\,\\textrm{d}x.$$\n\nFrom this and properties of summation and integration (see [Guide: Sigma notation](sigmanotation.qmd) and \\[Guide: Properties of integration\\]) it follows that\n\n2.  If $a,b$ are real number constants, then $$\\mathbb{E}(aX + b) = a\\mathbb{E}(X) + b.$$\n\n3.  If $Y$ is another random variable of the same type as $X$ (discrete or continuous), then $$\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y).$$\n\n4.  If $Y$ is a random variable that is **independent** of $X$, then $$\\mathbb{E}(XY) = \\mathbb{E}(X)\\mathbb{E}(Y).$$\n:::\n\n::: callout-tip\nProperties 3 and 4 can be extended to any number of random variables (all pairwise independent in the case of 4.) For instance, if $X_i$ are all random variables (of the same type) for $i = 1,\\ldots,n$, then $$\\mathbb{E}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\mathbb{E}(X_i).$$\n:::\n\nYou can use the properties of the expected value to find the expected value of some more probability distributions.\n\n::: {.callout-note appearance=\"simple\"}\n## Example 3\n\nAs seen in [Guide: PMFs, PDFs, CDFs](pmfspdfscdfs.qmd), a common example of a PMF is that of the **binomial distribution**. This is a type of PMF used to count the number of successes in a series of trials with only two possible outcomes: a success with probability $p$, or a failure with probability $q = 1 - p$. Here, the random variable $X$ is 'number of successes'. Take $x$ to be the number of successes in a number $n$ of trials, $p$ is the probability of success in a single trial, and $q = 1 - p$ is the probability of failure. Then the PMF $f(x)$ for a binomial distribution is given by:\n\n$$\nf(x) = \\binom{n}{x} p^x q^{(n-x)}\n$$\n\nHow would you work out $\\mathbb{E}(X)$ with this as the PMF? Instead, what you could do is use the fact that $X$ is a random variable that encapsulates the repetition of $n$ Bernoulli trials (see Example 2). So let $Y$ be the random variable describing a Bernoulli trial; it follows that $X = nY$. You can then use Property 2 of the expected value to say that $$\\mathbb{E}(X) = \\mathbb{E}(nY) = n\\mathbb{E}(Y).$$ You know that $\\mathbb{E}(Y) = p$ from Example 2, and so $$\\mathbb{E}(X) = n\\mathbb{E}(Y) = np.$$\n:::\n\n# Variance and standard deviation {.unnumbered}\n\nThe expected value is the 'central point' of the probability distribution, which is only one of the key measures of the probability distribution. The other is how the probability is spread about this central point. This is known as the **variance** of the distribution. Related to the variance is the **standard deviation**.\n\nGenerally, the larger the variance, the more the data is spread out about the central point. Let's take a look at the definition of the variance:\n\n::: {.callout-note icon=\"true\"}\n## Definition of variance and standard deviation\n\nSuppose that you have a random variable $X$ with expected value/mean $\\mu = \\mathbb{E}(X)$. The **variance** $\\mathbb{V}(X)$ of $X$ is given by $$\\mathbb{V}(X) = \\mathbb{E}\\left((X-\\mu)^2\\right)$$ that is, the expected value (average) of the squared deviation of the random variable $X$ from the mean.\n\nUsing the fact that $\\mu = \\mathbb{E}(X)$ and properties of expected value, you can rephrase the variance by the formula $$\\mathbb{V}(X) = \\mathbb{E}\\left(X^2\\right) - \\mathbb{E}\\left(X\\right)^2$$\n\nThe **standard deviation** $\\sigma$ is the square root of the variance; that is $$\\sigma = \\sqrt{\\mathbb{V}(X)}.$$ The standard deviation is so important that the variance is often written $\\sigma^2$.\n:::\n\n::: callout-tip\nMuch like sample means and population means, there is a concept of **sample variance** as well, to help line up distributions of samples compared to the overall distribution of the populations. See [Guide: Introduction to hypothesis testing](hypothesistesting.qmd) for more.\n:::\n\n::: {.callout-note appearance=\"simple\"}\n## Example 4\n\nWhat is the variance and standard deviation of the PMF for rolling a fair die? You could use the second formula for variance to work this out. You know from Example 1 that $\\mathbb{E}(X) = 7/2 = 3.5$.\n\nNow, you will need to work out $\\mathbb{E}(X^2)$. You can use the PMF to write $x^2$ for every $x$, and then use this to work out $\\mathbb{E}(X^2)$.\n\n| $x$   | 1   | 2   | 3   | 4   | 5   | 6   |\n|-------|-----|-----|-----|-----|-----|-----|\n| $x^2$ | 1   | 4   | 9   | 16  | 25  | 36  |\n\nso $\\mathbb{E}(X^2)$ is $$\\begin{aligned}\\mathbb{E}(X^2) &= 1\\cdot \\frac{1}{6} + 4\\cdot \\frac{1}{6} + 9\\cdot \\frac{1}{6} + 16\\cdot \\frac{1}{6} + 25\\cdot \\frac{1}{6} + 36\\cdot \\frac{1}{6}\\\\[1em] &= \\dfrac{1 + 4 + 9 + 16 + 25 + 36}{6} = \\frac{91}{6}\\end{aligned}$$ You can now use this to work out the variance of the random variable: $$\\begin{aligned}\\mathbb{V}(X) &= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\\\\[1em] &= \\frac{91}{6} - \\left(\\frac{7}{2}\\right)^2 = \\frac{91}{6} - \\frac{49}{4} = \\frac{35}{12}\\end{aligned}$$ Finally, the standard deviation is given by the square root of the variance, which means that $\\sigma = \\sqrt{35/12} \\approx 1.708.$\n\n:::\n\n::: {.content-visible when-format=\"html\"}\n\n::: {.callout-note appearance=\"simple\"}\n## Example 5\n\nConsider a continuous random variable $X$ uniformly distributed between $a$ and $b$, which you have seen in [Guide: PMFs, PDFs, CDFs](pmfspdfscdfs.qmd). The PDF of $X$ was given by:\n\n$$f(x) =\\begin{cases}\\dfrac{1}{b-a} & \\textsf{if } a \\leq x \\leq b \\\\[0.5em]0 & \\textsf{otherwise} \\end{cases}$$\n\nLet's find the expected value and variance of the uniform distribution.\n\nHere, you can use the formula for the expected value (together with properties of integration, the definition of the probability density function, and the difference of two squares) to get $$\\begin{aligned} \\mathbb{E}(X) &= \\int_{-\\infty}^\\infty xf(x)\\, \\textrm{d}x\\\\[1em] &= \\int_{-\\infty}^a x\\cdot 0\\, \\textrm{d}x + \\int_{a}^b \\frac{x}{b-a}\\, \\textrm{d}x + \\int_{b}^\\infty x\\cdot 0\\, \\textrm{d}x\\\\[1em] &= 0 + \\left[\\frac{x^2}{2(b-a)}\\right]_a^b + 0 = \\frac{b^2 - a^2}{2(b-a)} = \\frac{1}{2}(a+b)\\end{aligned}$$ This is what you would expect; it is the exact midpoint of the interval $[a,b]$!\n\nTo find the variance, you will need to find $\\mathbb{E}(X^2)$. You can modify the above working to get $$\\begin{aligned} \\mathbb{E}(X^2) &= \\int_{-\\infty}^\\infty x^2f(x)\\, \\textrm{d}x\\\\[1em] &= \\int_{-\\infty}^a x^2\\cdot 0\\, \\textrm{d}x + \\int_{a}^b \\frac{x^2}{b-a}\\, \\textrm{d}x + \\int_{b}^\\infty x^2\\cdot 0\\, \\textrm{d}x\\\\[1em] &= 0 + \\left[\\frac{x^3}{3(b-a)}\\right]_a^b + 0 = \\frac{b^3 - a^3}{2(b-a)} = \\frac{1}{3}(b^2 + ab + a^2)\\end{aligned}$$ Finally, you can say that $$\\begin{aligned}\\mathbb{V}(X) &= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\\\\ &= \\frac{1}{3}(b^2+ab + a^2) - \\left(\\frac{1}{2}(b+a)\\right)^2\\\\[0.5em] &= \\frac{1}{3}(b^2+ab + a^2) - \\frac{1}{4}(b^2 + 2ab + a^2)\\\\[0.5em] &= \\frac{1}{12}(b^2 - 2ab + a^2) = \\frac{1}{12}(b-a)^2\\end{aligned}$$ and this is the variance.\n:::\n\n:::\n\n::: {.content-hidden when-format=\"html\"}\n\n::: {.callout-note appearance=\"simple\"}\n## Example 5\n\nConsider a continuous random variable $X$ uniformly distributed between $a$ and $b$, which you have seen in [Guide: PMFs, PDFs, CDFs](pmfspdfscdfs.qmd). The PDF of $X$ was given by:\n\n$$f(x) =\\begin{cases}\\dfrac{1}{b-a} & \\textsf{if } a \\leq x \\leq b \\\\[0.5em]0 & \\textsf{otherwise} \\end{cases}$$\n\nLet's find the expected value and variance of the uniform distribution.\n\n:::\n\n \n\n::: {.callout-note appearance=\"simple\"}\n## Example 5 (continued)\n\nHere, you can use the formula for the expected value (together with properties of integration, the definition of the probability density function, and the difference of two squares) to get $$\\begin{aligned} \\mathbb{E}(X) &= \\int_{-\\infty}^\\infty xf(x)\\, \\textrm{d}x\\\\[1em] &= \\int_{-\\infty}^a x\\cdot 0\\, \\textrm{d}x + \\int_{a}^b \\frac{x}{b-a}\\, \\textrm{d}x + \\int_{b}^\\infty x\\cdot 0\\, \\textrm{d}x\\\\[1em] &= 0 + \\left[\\frac{x^2}{2(b-a)}\\right]_a^b + 0 = \\frac{b^2 - a^2}{2(b-a)} = \\frac{1}{2}(a+b)\\end{aligned}$$ This is what you would expect; it is the exact midpoint of the interval $[a,b]$!\n\nTo find the variance, you will need to find $\\mathbb{E}(X^2)$. You can modify the above working to get $$\\begin{aligned} \\mathbb{E}(X^2) &= \\int_{-\\infty}^\\infty x^2f(x)\\, \\textrm{d}x\\\\[1em] &= \\int_{-\\infty}^a x^2\\cdot 0\\, \\textrm{d}x + \\int_{a}^b \\frac{x^2}{b-a}\\, \\textrm{d}x + \\int_{b}^\\infty x^2\\cdot 0\\, \\textrm{d}x\\\\[1em] &= 0 + \\left[\\frac{x^3}{3(b-a)}\\right]_a^b + 0 = \\frac{b^3 - a^3}{2(b-a)} = \\frac{1}{3}(b^2 + ab + a^2)\\end{aligned}$$ Finally, you can say that $$\\begin{aligned}\\mathbb{V}(X) &= \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\\\\ &= \\frac{1}{3}(b^2+ab + a^2) - \\left(\\frac{1}{2}(b+a)\\right)^2\\\\[0.5em] &= \\frac{1}{3}(b^2+ab + a^2) - \\frac{1}{4}(b^2 + 2ab + a^2)\\\\[0.5em] &= \\frac{1}{12}(b^2 - 2ab + a^2) = \\frac{1}{12}(b-a)^2\\end{aligned}$$ and this is the variance.\n\n:::\n\n:::\n\nFinally, the variance has properties like the expected value does:\n\n::: {.callout-note icon=\"true\"}\n## Properties of variance\n\nLet $X$ be a random variable (either discrete or continuous). Then the following properties hold for the variance of $X$:\n\n1.  $\\mathbb{V}(X)\\geq 0$.\n\n2.  If $a$ is a real number then $$\\mathbb{V}(aX) = a^2\\mathbb{V}(X).$$\n\n3.  $\\mathbb{V}(X + b) = \\mathbb{V}(X)$ for all real numbers $b$.\n\n4.  If $Y$ is another random variable of the same type as $X$ (discrete or continuous), and $Y$ is **independent** of $X$, then $$\\mathbb{V}(X + Y) = \\mathbb{V}(X) + \\mathbb{V}(Y).$$\n:::\n\n::: callout-tip\nProperty 4 can be extended to any number of pairwise independent random variables (all pairwise independent in the case of 4.) So if $X_i$ are all pairwise independent random variables (of the same type) for $i = 1,\\ldots,n$, then $$\\mathbb{V}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\mathbb{V}(X_i).$$\n:::\n\n# Geometric interpretations of the expected value and variance\n\n::: {.content-visible when-format=\"html\"}\nYou can use the interactive calculators below to explore the normal distribution, exponential distribution, and beta distribution (for more, see [Overview: Probability distributions](../overviews/o-distributions.qmd)) by changing the mean $\\mu$ and the variance $\\sigma^2$ by using the sliders. You can see that changing the mean changes the centre of the curve. Increasing the variance leads to the curve being more spread out. Decreasing the variance makes the curve far more narrow - as the data are more clustered toward the mean with a smaller variance.\n\n## Normal distribution\n\nThe normal distribution has mean $\\mu$ and variance $\\sigma^2$, where the probability density function is given by $$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left({-\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2}\\right).$$\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 770\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\n\nui <- page_fluid(\n  title = \"Normal distribution visualizer\",\n  \n  # Plot at the top\n  card(\n    card_header(\"Normal distribution plot\"),\n    card_body(\n      plotOutput(\"distPlot\", height = \"500px\")\n    )\n  ),\n  \n  # Parameters below\n  card(\n    card_header(\"Distribution parameters\"),\n    card_body(\n      layout_columns(\n        col_widths = c(6, 6),\n        sliderInput(\"mean\", \"Mean/expected value (μ):\", \n                   min = -5, max = 5, value = 0, step = 0.1),\n        sliderInput(\"variance\", \"Variance (σ²):\", \n                   min = 0.25, max = 9, value = 1, step = 0.25)\n      )\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  # Generate the normal distribution plot\n  output$distPlot <- renderPlot({\n    # Fixed range for x-axis to show consistent scale\n    x_min <- -8\n    x_max <- 8\n    \n    # Convert variance to standard deviation\n    sd_val <- sqrt(input$variance)\n    \n    # Create data frame for plotting\n    x <- seq(x_min, x_max, length.out = 500)\n    y <- dnorm(x, mean = input$mean, sd = sd_val)\n    df <- data.frame(x = x, y = y)\n    \n    # Create plot with fixed axes\n    p <- ggplot(df, aes(x = x, y = y)) +\n      geom_line(color = \"#3F6BB6\", size = 1.2) +\n      labs(x = \"X\", y = \"Density\",\n           title = sprintf(\"Normal distribution: N(μ = %.1f, σ² = %.2f)\", \n                          input$mean, input$variance)) +\n      theme_minimal() +\n      theme(\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5, size = 16),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12)\n      ) +\n      # Fixed axis limits\n      xlim(x_min, x_max) +\n      ylim(0, 0.8) +\n      # Add reference line at x = 0\n      geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\", alpha = 0.7) +\n      # Add reference line at mean\n      geom_vline(xintercept = input$mean, linetype = \"dashed\", color = \"#DB4315\", alpha = 0.7) +\n      annotate(\"text\", x = input$mean + 0.8, y = 0.7, label = \"Mean\", color = \"#DB4315\", size = 4)\n    \n    return(p)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n## Exponential distribution\n\nThe exponential distribution has probability density function $$f(x) = \\lambda e^{-\\lambda x},$$ where $\\lambda$ is number of times an event occurs within a specific period of time. (For more on this, see [Overview: Probability distributions](../overviews/o-distributions.qmd) It can be shown that the mean/expected value of the exponential distribution is $\\mu = 1/\\lambda$ and variance $\\sigma^2 = 1/\\lambda^2$, and so both the expected value and variance depend on $\\lambda$ - which explains the one slider.\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 770\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\n\nui <- page_fluid(\n  title = \"Exponential distribution visualizer\",\n  \n  # Plot at the top\n  card(\n    card_header(\"Exponential distribution plot\"),\n    card_body(\n      plotOutput(\"distPlot\", height = \"500px\")\n    )\n  ),\n  \n  # Parameters below\n  card(\n    card_header(\"Distribution parameters\"),\n    card_body(\n      sliderInput(\"rate\", \"Rate parameter (λ):\", \n                 min = 0.1, max = 3, value = 1, step = 0.1)\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  # Generate the exponential distribution plot\n  output$distPlot <- renderPlot({\n    # Fixed range for x-axis to show consistent scale\n    x_min <- 0\n    x_max <- 10\n    \n    # Create data frame for plotting\n    x <- seq(x_min, x_max, length.out = 500)\n    y <- dexp(x, rate = input$rate)\n    df <- data.frame(x = x, y = y)\n    \n    # Calculate mean and variance for display\n    mean_val <- 1 / input$rate\n    variance_val <- 1 / (input$rate^2)\n    \n    # Create plot with fixed axes\n    p <- ggplot(df, aes(x = x, y = y)) +\n      geom_line(color = \"#3F6BB6\", size = 1.2) +\n      labs(x = \"X\", y = \"Density\",\n           title = sprintf(\"Exponential distribution: Exp(λ = %.1f)\\nMean = %.2f, Variance = %.2f\", \n                          input$rate, mean_val, variance_val)) +\n      theme_minimal() +\n      theme(\n        panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5, size = 16),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12)\n      ) +\n      # Fixed axis limits\n      xlim(x_min, x_max) +\n      ylim(0, 3) +\n      # Add reference line at mean\n      geom_vline(xintercept = mean_val, linetype = \"dashed\", color = \"#DB4315\", alpha = 0.7) +\n      annotate(\"text\", x = mean_val + 0.5, y = 2.5, label = \"Mean\", color = \"#DB4315\", size = 4)\n    \n    return(p)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n## Beta distribution\n\nThe beta distribution is used to model probabilities. It has probability density function $$f(x) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\textrm{B}(\\alpha,\\beta)},$$ where $\\textrm{B}$ is the beta function and $\\alpha = k+1$ and $\\beta = n - k + 1$ are parameters for measuring the number of trials ($k$) and the number of success of those trials ($n$). It can be shown that the mean/expected value of the beta distribution is $\\mu = \\dfrac{\\alpha}{\\alpha + \\beta}$ and variance $\\sigma^2 = \\dfrac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$.\n\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 770\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(ggplot2)\n\nui <- page_fluid(\n  title = \"Beta distribution\",\n  \n  # Plot at the top\n  card(\n    card_header(\"Beta distribution plot\"),\n    card_body(\n      uiOutput(\"plot_title\"),\n      plotOutput(\"distPlot\", height = \"400px\")\n    )\n  ),\n  \n  # Parameters below\n  card(\n    card_header(\"Distribution parameters\"),\n    card_body(\n      layout_columns(\n        col_widths = c(6, 6),\n        sliderInput(\"mean\", \"Mean:\", min = 0.1, max = 0.9, value = 0.5, step = 0.01),\n        sliderInput(\"variance\", \"Variance:\", min = 0.01, max = 0.2, value = 0.05, step = 0.01)\n      )\n    )\n  )\n)\n\nserver <- function(input, output, session) {\n  \n  # Convert mean and variance to shape parameters\n  beta_params <- reactive({\n    mean_val <- input$mean\n    var_val <- input$variance\n    \n    # For a beta distribution: mean = α/(α+β), var = αβ/((α+β)²(α+β+1))\n    # Solving for α and β given mean and variance:\n    # α = mean * ((mean * (1 - mean)) / variance - 1)\n    # β = (1 - mean) * ((mean * (1 - mean)) / variance - 1)\n    \n    # Ensure variance doesn't exceed theoretical maximum\n    max_var <- mean_val * (1 - mean_val) / 1.01  # Small buffer\n    if (var_val >= max_var) {\n      var_val <- max_var\n    }\n    \n    common_term <- (mean_val * (1 - mean_val)) / var_val - 1\n    alpha <- mean_val * common_term\n    beta <- (1 - mean_val) * common_term\n    \n    # Ensure positive parameters\n    alpha <- max(alpha, 0.01)\n    beta <- max(beta, 0.01)\n    \n    return(list(alpha = alpha, beta = beta))\n  })\n  \n  # Display the plot title with distribution parameters\n  output$plot_title <- renderUI({\n    params <- beta_params()\n    title <- sprintf(\"Beta(α = %.2f, β = %.2f)\", params$alpha, params$beta)\n    subtitle <- sprintf(\"Mean = %.2f, Variance = %.4f\", input$mean, input$variance)\n    div(\n      tags$h4(title, style = \"text-align: center; margin-bottom: 5px;\"),\n      tags$p(subtitle, style = \"text-align: center; color: gray; margin-bottom: 15px;\")\n    )\n  })\n  \n  # Generate the beta distribution plot\n  output$distPlot <- renderPlot({\n    # Get shape parameters\n    params <- beta_params()\n    shape1 <- params$alpha\n    shape2 <- params$beta\n    \n    # Create data frame for plotting with fixed x range\n    x_values <- seq(0, 1, length.out = 500)\n    density_values <- dbeta(x_values, shape1 = shape1, shape2 = shape2)\n    plot_df <- data.frame(x = x_values, density = density_values)\n    \n    # Create plot with fixed axes\n    p <- ggplot(plot_df, aes(x = x, y = density)) +\n      geom_line(size = 1, color = \"#3F6BB6\") +\n      geom_area(aes(x = x, y = density), fill = \"#3F6BB6\", alpha = 0.3) +\n      labs(x = \"X\", y = \"Probability Density\") +\n      theme_minimal() +\n      theme(panel.grid.minor = element_blank()) +\n      xlim(0, 1) +\n      ylim(0, 10) +  # Fixed y-axis range\n      # Add reference line at mean\n      geom_vline(xintercept = input$mean, linetype = \"dashed\", color = \"#DB4315\", alpha = 0.7) +\n      annotate(\"text\", x = input$mean + 0.08, y = 8.5, label = \"Mean\", color = \"#DB4315\", size = 4)\n    \n    return(p)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n```\n\n:::\n\n::: {.content-hidden when-format=\"html\"}\nThe figures below explore the normal distribution (for more, see [Overview: Probability distributions](../overviews/o-distributions.qmd)) by changing the mean $\\mu$ and the variance $\\sigma^2$. \n\nFirst of all, here is a figure of the normal distribution with mean $\\mu = 0$ and variance $\\sigma^2 = 1$:\n\n![The normal distribution with mean $\\mu = 0$ and variance $\\sigma^2 = 1$.](./FiguresPNG/expectedvariance-fig1.png){width=\"50%\"}\n\n\nNext, here is a figure of the normal distribution with mean $\\mu = -2$ and variance $\\sigma^2 = 1$. You can see that changing the mean changes the centre of the curve. \n\n![The normal distribution with mean $\\mu = -2$ and variance $\\sigma^2 = 1$.](./FiguresPNG/expectedvariance-fig2.png){width=\"50%\"}\n\nThirdly, here is a figure of the normal distribution with mean $\\mu = -2$ and variance $\\sigma^2 = 0.5$. You can see that decreasing the variance makes the curve far more narrow - as the data are more clustered toward the mean with a smaller variance.\n\n![The normal distribution with mean $\\mu = -2$ and variance $\\sigma^2 = 0.5$.](./FiguresPNG/expectedvariance-fig3.png){width=\"50%\"}\n\nFinally, here is a figure of the normal distribution with mean $\\mu = -2$ and variance $\\sigma^2 = 4$. You can see that increasing the variance leads to the curve being more spread out around the mean. \n\n![The normal distribution with mean $\\mu = -2$ and variance $\\sigma^2 = 4$.](./FiguresPNG/expectedvariance-fig4.png){width=\"50%\"}\n\nYou can see dynamic versions of these figures on the html version of this guide.\n\n:::\n\n# Quick check problems\n\n:::: {.content-visible when-format=\"html\"}\n\n::: {.webex-check .webex-box data-topic=\"EVVSD1\"}\n\n1.  Are the following statements true or false?\n\n<!-- -->\n\n<!-- (a) The sample mean ($\\bar{x}$) is always equal to the population mean $\\mu$: TRUE / FALSE -->\n\n(a) The variance is a measure of how data is dispersed around the expected value: TRUE / FALSE\n\n(b) The standard deviation is the square of the variance: TRUE / FALSE\n\n(c) The expected value is the 'central point' of the distribution: TRUE / FALSE\n\n<!-- -->\n\n2.  Find the expected value and variance for rolling a fair 4-sided die. You should give your answers as decimal numbers.\n\nThe expected value is $\\mathbb{E}(X) =$ ___ and the variance is $\\mathbb{V}(X) =$ ____.\n\n3.  Using the results from Examples 1 and 4 and the properties of expected values and variance, find the expected value and variance for rolling three fair 6-sided dice. You can assume that the events are independent of each other. You should give your answers as decimal numbers.\n\nThe expected value is $\\mu =$ ____ and the variance is $\\sigma^2 =$ ____.\n:::\n::::\n\n::: {.content-hidden when-format=\"html\"}\n1.  Are the following statements true or false?\n\n<!-- -->\n\n(a) The variance is a measure of how data is dispersed around the expected value:.\n\n(b) The standard deviation is the square of the variance.\n\n(c) The expected value is the 'central point' of the distribution.\n\n<!-- -->\n\n2.  Find the expected value and variance for rolling a fair 4-sided die.\n\n3.  Using the results from Examples 1 and 4 and the properties of expected values and variance, find the expected value and variance for rolling three fair 6-sided dice. You can assume that the events are independent of each other.\n:::\n\n# Further reading {.unnumbered}\n\n[For more questions on the subject, please go to Questions: Expected value, variance, standard deviation.](../questions/qs-expectedvariance.qmd)\n\n[For more on why some PMFs and PDFs are valid, please go to Proof sheet: PMFs, PDFs, CDFs.](../proofsheets/ps-pmfspdfscdfs.qmd)\n\nFor more on probability distributions see [Overview: Probability distributions.](../overviews/o-distributions.qmd)\n\n## Version history and licensing {.unnumbered}\n\nv1.0: initial version created 08/25 by tdhc.\n\n[This work is licensed under CC BY-NC-SA 4.0.](https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}