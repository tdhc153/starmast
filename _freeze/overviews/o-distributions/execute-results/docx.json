{
  "hash": "d266a9fc04c2f919cc87abb69cb7d7a7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Overview: Distributions\"\nauthor: Michelle Arnetta\nabstract-title: Summary\nabstract: An overview of different types of distributions for both continuous random variables and discrete random variables.\ncategories:\ndraft: true\n---\n\n\n\n\n\n\n\n\n\n\n# How to use\n\nThis overview will contain the following content on each distribution:\n\n-   Notation\n\n-   Where to use\n\n-   Mean and Expected Value (the expected value of $X$ will be represented as $E(X)$)\n\n-   Variance (the variance of $X$ will be represented as $Var(X)$)\n\n-   Probability Mass Function (PMF) for discrete random variables, or Probability Density Function (PDF) for continuous random variables\n\n-   Cumulative Distribution Function (CDF)\n\n-   Interactive Figure\n\n-   Example\n\nAlthough it is not an exhaustive list, this overview can be treated as an introduction to commonly used types of distributions.\n\n# Discrete Random Variables\n\n## Bernoulli Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Bernoulli(p)$\n\nParameter:\n\n-   $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Where to use**\n\nThe Bernoulli distribution is used for binary data, where one trial is conducted with only two possible outcomes. Examples include success/failure and Yes/No. $X$ indicates whether the trial is a success (when $X=1$) or failure (when $X=0$).\n\n**Mean and Expected Value**\n\n$E(X) = p$\n\n**Variance**\n\n$Var(X)=p(1-p)$\n\n**PMF**\n\n$P(X=x)=\\begin{cases} 1-p & x=0 \\\\p & x=1\\end{cases}$\n\n**CDF**\n\n$P(X\\leq x)= \\begin{cases} 0 & x< 0 \\\\1-p & 0\\leq x<1 \\\\p & x\\geq1 \\end{cases}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou flip a coin, and the probability of getting ‘heads’ is 0.5. Taking ‘heads’ as a success, this can be expressed as $X \\sim Bernoulli(0.5)$, meaning the probability of success in each trial is 0.5.\n\n## Binomial Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Binomial(n,p)$ or $X \\sim B(n,p)$\n\nParameters:\n\n-   $n$ = number of trials\n-   $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Where to use**\n\nThe binomial distribution is used when there are a fixed number of trials ($n$) and only two possible outcomes for each trial. $X$ represents the number of successes.\n\n**Mean and Expected Value**\n\n$E(X) = np$\n\n**Variance**\n\n$Var(X) = np(1-p)$\n\n**PMF**\n\n$P(X=x)=\\frac{n!}{(n-x)!x!}p^xq^{(n-x)}$\n\n**CDF**\n\n$P(X\\leq x)=\\sum^{x}_{i=1}\\frac{n!}{(n-x)!x!}p^xq^{(n-x)}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou flip a coin 10 times, and the probability of getting ‘heads’ is 0.5. Taking ‘heads’ as a success, this can be expressed as $X \\sim B(10, 0.5)$, meaning 10 trials are conducted, where the probability of success in each trial is 0.5.\n\n## Multinomial Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Multinomial(n,p)$ or $X \\sim M(n,p)$\n\nParameters:\n\n-   $n$ = number of trials\n-   $p$ = vector with probabilities of each outcome (expressed as $p_{1},...,p_{k}$ where $k$ = number of possible mutually exclusive outcomes)\n\n**Where to use**\n\nThe multinomial distribution is used when there are a fixed number of trials ($n$) and more than two possible outcomes for each trial. $X_{i}$ represents the number of times a specific outcome occurs. Therefore, the mean, variance, and expected value of multinomial distributions are calculated for each $X_{i}$, not $X$.\n\n**Mean and Expected Value**\n\n$E(X_{i}) = np_{i}$\n\n**Variance**\n\n$Var(X_{i}) = np_{i}(1-p_{i})$\n\n**PMF**\n\n$P(X_{1}=x_{i},...,X_{k}=x_{k})=\\frac{n!}{x_{i}!...x_{k}!}p^{x_{1}}...p^{x_{k}}$\n\n**CDF**\n\nIt is difficult to define CDF for the multinomial distribution.\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nThere is a candy jar consisting of 5 red candies, 3 blue candies, and 7 yellow candies.\n\n-   The probability of drawing a red candy is $\\frac{1}{3}$.\n\n-   The probability of drawing a blue candy is $\\frac{1}{5}$.\n\n-   The probability of drawing a yellow candy is $\\frac{7}{15}$\n\nYou draw 3 candies from the jar. This can be expressed as $X \\sim M(3,\\frac{1}{3},\\frac{1}{5},\\frac{7}{15})$. It means 3 trials are conducted, where $p_{1}=\\frac{1}{3}$, $p_{2}=\\frac{1}{5}$, and $p_{3}=\\frac{7}{15}$ (and $k=3$).\n\n## Poisson Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Poisson(\\lambda)$ or $X \\sim Pois(\\lambda)$\n\nParameter:\n\n-   $\\lambda$ = number of times an event occurs within a specific period of time\n\n**Where to use**\n\nThe Poisson distribution is used when a specific event occurs at some rate ($\\lambda$), and you are counting $X$, the number of times this event occurs in some interval.\n\n**Mean and Expected Value**\n\n$E(X) = \\lambda$\n\n**Variance**\n\n$Var(X) = \\lambda$\n\n**PMF**\n\n$P(X=x)=\\frac{\\lambda^ke^{-\\lambda}}{k!}$\n\n**CDF**\n\n$P(X \\leq x)=\\sum^{x}_{i=1}\\frac{\\lambda^ke^{-\\lambda}}{k!}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nCustomers enter Cantor’s Confectionery at an average rate of 20 people per hour. This can be expressed as $X \\sim Pois(20)$, meaning the event of customers entering the store occurs 20 times within an hour\n\n## Negative Binomial Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim NB(r,p)$\n\nParameters:\n\n-   $r$ = targeted number of successes\n-   $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Where to use**\n\nThe negative binomial distribution is used an alternative to the Poisson distribution, when the variance may exceed the mean. $X$ represents the number of trials required to reach the targeted number of successes ($r$).\n\n**Mean and Expected Value**\n\n$E(X) = \\frac{r(1-p)}{p}$\n\n**Variance**\n\n$Var(X) = \\frac{r(1-p)}{p^2}$\n\n**PMF**\n\n$P(X=x)=\\frac{(x+r-1)!}{(r-1)!x!}(1-p)^xp^r$\n\n**CDF**\n\n$P(X \\leq x)=\\sum^{x}_{i=1}\\frac{(x+r-1)!}{(r-1)!x!}(1-p)^xp^r$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou flip a coin multiple times, and the probability of getting ‘heads’ is 0.5. You decide to stop flipping the coin once you get 3 ‘heads’; these do not have to be consecutive. Taking ‘heads’ as a success, this can be expressed as $X \\sim NB(3,0.5)$. It means the probability of success is 0.5, and you will stop conducting trials after you reach 3 successes.\n\n## Geometric Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Geometric(p)$\n\nParameter:\n\n-   $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Where to use**\n\nThe geometric distribution is used to count $X$, the number of trials until a successful outcome is reached.\n\n**Mean and Expected Value**\n\n$E(X) = \\frac{1}{p}$\n\n**Variance**\n\n$Var(X) = \\frac{1-p}{p^2}$\n\n**PMF**\n\n$P(X=x)=(1-p)^{k-1}p$\n\n**CDF**\n\n$P(X \\leq x)=\\begin{cases} 1-(1-p)^x & x\\geq0 \\\\0 & x<1\\end{cases}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou flip a coin multiple times, and the probability of getting ‘heads’ is 0.5. You decide to stop flipping the coin once you get a ‘heads’. Taking ‘heads’ as a success, this can be expressed as $X \\sim Geometric(0.5)$. It means the probability of success is 0.5, and you will stop conducting trials after you reach a success.\n\n# Continuous Random Variables\n\n## Uniform Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Uniform(a,b)$ or $X \\sim U(a,b)$\n\nParameters:\n\n-   $a$=minimum value\n-   $b$=maximum value where all values of $X$ for $a \\leq x \\leq b$ are equally likely\n\n**Where to use**\n\nThe uniform distribution is used when all values in some interval ($a$ to $b$) are equally likely.\n\n**Mean and Expected Value**\n\n$E(X) = \\frac{a+b}{2}$\n\n**Variance**\n\n$Var(X) = \\frac{(b-a)^2}{12}$\n\n**PMF**\n\n$P(X=x)=\\begin{cases} \\frac{1}{b-a} & a \\leq x \\leq b \\\\0 & \\textrm{otherwise}\\end{cases}$\n\n**CDF**\n\n$P(X\\leq x)= \\begin{cases} 0 & x< a \\\\\\frac{x-a}{b-a} & a\\leq x\\leq b \\\\1 & x>b \\end{cases}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou roll a fair six-sided die, where all outcomes (1, 2, 3, 4, 5, and 6) are equally likely. This can be expressed as $X \\sim U(1,6)$. It means 1 is the minimum value and 6 is the maximum value, where all values of $X$ for $1 \\leq x \\leq 6$ are equally likely.\n\n## Exponential Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Exponential(\\lambda)$ or $X \\sim Exp(\\lambda)$\n\nParameter:\n\n-   $\\lambda$ = number of times an event occurs within a specific period of time\n\n**Where to use**\n\nThe exponential distribution is used when $X$ is the waiting time before a certain event occurs. It is similar to the geometric distribution, but the exponential distribution uses continuous waiting time instead of the integer number of trials.\n\n**Mean and Expected Value**\n\n$E(X) = \\frac{1}{\\lambda}$\n\n**Variance**\n\n$Var(X) = \\frac{1}{\\lambda^2}$\n\n**PMF**\n\n$P(X=x)=\\lambda e^{-\\lambda x}$\n\n**CDF**\n\n$P(X \\leq x)=1-e^{-\\lambda x}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nCustomers enter Cantor’s Confectionery at an average rate of 20 people per hour, and the time distance between each visit can be modeled by an exponential distribution. This can be expressed as $X \\sim Exp(20)$.\n\n## Gamma Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Gamma(\\alpha,\\theta)$ or $X \\sim Gam(\\alpha,\\theta)$\n\nParameters:\n\n-   $\\alpha = \\frac{\\mu^2}{\\sigma^2}$ (shape parameter)\n-   $\\theta=\\frac{\\sigma^2}{\\mu}$ (scale parameter) where $\\mu$ = mean and $\\sigma^2$ = variance\n\n**Where to use**\n\nThe gamma distribution generalizes the exponential distribution, allowing for greater or lesser variance. It is used to model positive continuous random variables that have skewed distributions.\n\n**Mean and Expected Value**\n\n$E(X) = \\alpha\\theta$\n\n**Variance**\n\n$Var(X) = \\alpha\\theta^2$\n\n**PMF**\n\n?\n\n**CDF**\n\n?\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou collect historical data on the time to failure of a machine from Cantor’s Confectionery. The mean is 83 days and the variance is 50.3. You can then use this to estimate the shape and scale parameters of the Gamma Distribution: - $\\alpha = \\frac{83^2}{50.3} = 136.958250497 \\approx 137$ - $\\theta = \\frac{50.3}{83} = 0.60602409638 \\approx 0.61$ The distribution can be expressed as $X \\sim Gam(137,0.61)$, where the shape parameter is 137 and the scale parameter is 0.61.\n\n## Beta Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Beta(\\alpha,\\beta)$\n\nParameters:\n\n-   $\\alpha = k + 1$ (shape parameter)\n-   $\\beta = n - k + 1$ (shape parameter) where $k$ = number of successes and $n$ = number of trials\n\n**Where to use**\n\nThe beta distribution is used to model the distribution of probabilities or proportions (numbers between 0 and 1).\n\n**Mean and Expected Value**\n\n$E(X) = \\frac{\\alpha}{\\alpha+\\beta}$\n\n**Variance**\n\n$Var(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$\n\n**PMF**\n\n?\n\n**CDF**\n\n?\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nCantor’s Confectionery is visited by 10 customers, and 6 of them purchase something from the store. Taking the buying customers as successes and the total visiting customers as number of trials, there would be 6 successes, allowing you to find the following parameters: - $\\alpha = 6 + 1 = 7$ - $\\beta = 10 - 6 + 1 = 5$ Then the distribution of the probabilities of a customer purchasing from Cantor’s Confectionery can be expressed as $X \\sim Beta(7,5)$,meaning the first shape parameter is 7 and the second shape parameter is 5.\n\n## Normal Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Normal(\\mu,\\sigma^2)$ or $X \\sim N(\\mu,\\sigma^2)$\n\nParameter:\n\n-   $\\mu$ = mean (location parameter)\n-   $\\sigma^2$ = variance (squared scale parameter), where $\\sigma$ = standard deviation (scale parameter)\n\n**Where to use**\n\nThe normal distribution is used to model continuous random variables that can have values anywhere on the real line (positive or negative). The use of this distribution is often justified by the Central Limit Theorem.\n\n**Mean and Expected Value**\n\n$E(X) = \\mu$\n\n**Variance**\n\n$Var(X) = \\sigma^2$\n\n**PMF**\n\n?\n\n**CDF**\n\n?\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nThe lengths of chocolate bars produced by Cantor’s Confectionery follow a normal distribution with a mean of 5.6 inches and a standard deviation of 1.2. This can be expressed as $X \\sim N(5.6, 1.2^2)$, meaning the data is normally distributed, centered at 5.6 (location parameter) and stretched by 1.2 (scale parameter).\n\n## Lognormal Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim Lognormal(\\mu,\\sigma^2)$\n\nParameter:\n\n-   $\\mu$ = logarithm of location parameter\n-   $sigma^2$ = logarithm of scale parameter\n\n**Where to use**\n\nThe lognormal distribution is used to model continuous random variables that can have values anywhere on the real line greater than or equal to zero, wherein the logarithms of these variables follow a normal distribution.\n\n**Mean and Expected Value**\n\n$E(X) = \\lambda$\n\n**Variance**\n\n$Var(X) = \\lambda$\n\n**PMF**\n\n$P(X=x)=exp(\\mu+\\frac{\\sigma^2}{2})$\n\n**CDF**\n\n$P(X \\leq x)=[exp(\\sigma^2)-1]exp(2\\mu+\\sigma^2)$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nThe logarithms of Cantor’s Confectionery’s stock prices follow a normal distribution. The mean of the stock prices’ natural logarithms is 8.01, whereas the variance of the stock prices’ natural logarithms is 3. This can be expressed as $X \\sim Lognormal(8.01, 3)$, meaning the logarithm of the location parameter is 8.01 and the logarithm of scale parameter is 3.\n\n## Chi-squared Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim \\chi^2(k)$\n\nParameter:\n\n-   $k$ = degrees of freedom\n\n**Where to use**\n\nThe $\\chi^2$ distribution is used for squared standardised normal random variables.\n\n**Mean and Expected Value**\n\n$E(X) = k$\n\n**Variance**\n\n$Var(X) = 2k$\n\n**PMF**\n\n?\n\n**CDF**\n\n?\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\n-   **Goodness of Fit Example:** You have a six-sided die with six possible outcomes: 1, 2, 3, 4, 5, and 6. You calculate the expected frequencies of each outcome. Then you roll the die many times and record the observed frequencies of each outcome. Since there are 6 categories, the degrees of freedom = number of categories - 1 = 6 - 1 = 5. This can be expressed as $X \\sim \\chi^2(5)$, meaning the degrees of freedom is 5.\n\n-   **Test for Independence Example:** You are investigating whether there is a correlation between two variables: candy color and flavor. You have 5 categories of colors and 3 categories of flavors. Calculating the degrees of freedom can be done with (number of rows - 1)(number of columns - 1) = (5-1)(3-1)=(4)(2)=8. Hence, $X \\sim \\chi^2(8)$, meaning the degrees of freedom is 8.\n\n## $F$ Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim F(d_{1},d_{2})$\n\nParameters:\n\n-   $d_{1}$ = numerator degrees of freedom\n-   $d_{2}$ = denominator degrees of freedom\n\n**Where to use**\n\nThe $F$ distribution is used for the ratio of two independent Chi-squared random variables.\n\n**Mean and Expected Value**\n\n$E(X) = \\frac{d_{2}}{d_{2}-2}$ for $d_{2}>2$\n\n**Variance**\n\n$Var(X) = \\frac{2d_{2}(d_{1}+d_{2}-2)}{d_{1}(d_{2}-2)^2(d_{2}-4)}$\n\n**PMF**\n\n?\n\n**CDF**\n\n?\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou have three independent groups of data containing Cantor’s Confectionery chocolate bar lengths, and the total sample size is 90. From this, you would like to conduct an ANOVA test investigating if there is a statistically significant difference between each group’s means. You find the degrees of freedom using the following methods: - numerator degrees of freedom = number of groups - 1 = 3 - 1 = 2 - denominator degrees of freedom = sample size - number of groups = 90 - 3 = 87 The F distribution, which will be used as a reference distribution for the ANOVA test, can be expressed as $X \\sim F(2,87)$, meaning the numerator degrees of freedom is 2 and the denominator degrees of freedom is 87.\n\n## $t$ Distribution {.unnumbered}\n\n**Notation**\n\n$X \\sim t(v)$\n\nParameter:\n\n-   $v$ = degrees of freedom\n\n**Where to use**\n\nThe $t$ distribution is a special case of the $F$ distribution, and it is used for continuous random variables with heavier tails than the Normal distribution.\n\n**Mean and Expected Value**\n\n$E(X) = 0$\n\n**Variance**\n\n$Var(X) = \\frac{v}{v-2}$ for $v>2$\n\n**PMF**\n\n?\n\n**CDF**\n\n?\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example**\n\nYou have a sample of 40 measurements of Cantor’s Confectionery chocolate bar lengths. From this, you would like to conduct a one sample t-test comparing the sample to a hypothesized mean. You find the degrees of freedom = sample size - 1 = 40 - 1 = 39. The t distribution, which will be used as a reference distribution for the t-test, can be expressed as $X \\sim t(39)$, meaning the degrees of freedom is 39.\n\n# Further reading {.unnumbered}\n\n## Version history {.unnumbered}\n\nv1.0: initial version created\n\n[This work is licensed under CC BY-NC-SA 4.0.](https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}