{
  "hash": "6dceed512097a442da3741622b9a3c40",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Overview: Probability Distributions\"\nauthor: Michelle Arnetta\nabstract-title: Summary\nabstract: An overview of different types of distributions for both continuous random variables and discrete random variables.\ndraft: true\n---\n\n\n\n\n\n\n\n# How to use\n\nThis overview will contain the following content on each distribution:\n\n-   Where to use\n\n-   Notation\n\n-   Parameter(s)\n\n-   Mean and Expected Value (mean = expected value, and the expected value of $X$ will be represented as $E(X)$)\n\n-   Variance (the variance of $X$ will be represented as $\\textrm{Var}(X)$)\n\n-   Probability Mass Function (PMF) for discrete random variables, or Probability Density Function (PDF) for continuous random variables\n\n-   Cumulative Distribution Function (CDF), where possible\n\n-   Interactive Figure, where possible\n\n-   Example(s)\n\nAlthough it is not an exhaustive list, this overview can be treated as an introduction to commonly used types of distributions.\n\n# Discrete Random Variables\n\n## Uniform Distribution (Discrete) {.unnumbered}\n\n**Where to use:** The discrete uniform distribution is used when all discrete values in the interval $a$ to $b$ are equally likely.\n\n**Notation:** $X \\sim \\textrm{Uniform}(a,b)$ or $X \\sim U(a,b)$\n\n**Parameters:**\n\n-   $a$ = minimum value\n-   $b$ = maximum value where all values of $X$ for $a \\leq x \\leq b$ are equally likely\n\n**Mean and Expected Value:** $E(X) = \\frac{a+b}{2}$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{(b-a+1)^2-1}{12}$\n\n**PMF:** $P(X=x)=\\frac{1}{n}$ where $n=b-a+1$\n\n**CDF:** $P(X\\leq x)= \\begin{cases} 0 & \\textsf{if } x \\leq a \\\\\\frac{\\lfloor x \\rfloor - a + 1}{n} & \\textsf{if } a< x<b \\\\1 & \\textsf{if } x \\geq b \\end{cases}$\n\nwhere $n=b-a+1$ and $\\lfloor x \\rfloor$ is $x$ rounded down to the nearest integer\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You roll a fair six-sided die, where all outcomes (1, 2, 3, 4, 5, and 6) are equally likely. This can be expressed as $X \\sim U(1,6)$. It means 1 is the minimum value and 6 is the maximum value, where all discrete values of $X$ for $1 \\leq x \\leq 6$ are equally likely.\n\n## Bernoulli Distribution {.unnumbered}\n\n**Where to use:** The Bernoulli distribution is used for binary data, where one trial is conducted with only two possible outcomes. Examples include success/failure, yes/no, and heads/tails. $X$ indicates whether the trial is a success (when $X=1$) or failure (when $X=0$).\n\n**Notation:** $X \\sim \\textrm{Bernoulli}(p)$\n\n**Parameter:** $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Mean and Expected Value:** $E(X) = p$\n\n**Variance:** $\\textrm{Var}(X)=p(1-p)$\n\n**PMF:** $P(X=x)=\\begin{cases} 1-p & \\textsf{if }x=0 \\\\p & \\textsf{if }x=1\\end{cases}$\n\n**CDF:** $P(X\\leq x)= \\begin{cases} 0 & \\textsf{if } x< 0 \\\\1-p & \\textsf{if } 0\\leq x<1 \\\\p & \\textsf{if } x\\geq1 \\end{cases}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You flip a coin, and the probability of getting ‘heads’ is 0.5. Taking ‘heads’ as a success, this can be expressed as $X \\sim \\textrm{Bernoulli}(0.5)$, meaning the probability of success in each trial is 0.5.\n\n## Binomial Distribution {.unnumbered}\n\n**Where to use:** The binomial distribution is used when there are a fixed number of trials ($n$) and only two possible outcomes for each trial. $X$ represents the number of successes.\n\n**Notation:** $X \\sim \\textrm{Binomial}(n,p)$ or $X \\sim B(n,p)$\n\n**Parameters:**\n\n-   $n$ = number of trials\n-   $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Mean and Expected Value:** $E(X) = np$\n\n**Variance:** $\\textrm{Var}(X) = np(1-p)$\n\n**PMF:** $P(X=x)=\\frac{n!}{(n-x)!x!}p^xq^{(n-x)}$\n\n**CDF:** $P(X\\leq x)=I_{q}(n-\\lfloor x \\rfloor,1+\\lfloor x \\rfloor)$ where $I_{x}(a,b)$ is the regularized incomplete beta function for $a$ and $b$ with $x$ being the upper bound for integration\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You flip a coin 10 times, and the probability of getting ‘heads’ is 0.5. Taking ‘heads’ as a success, this can be expressed as $X \\sim B(10, 0.5)$, meaning 10 trials are conducted, where the probability of success in each trial is 0.5.\n\n## Multinomial Distribution {.unnumbered}\n\n**Where to use:** The multinomial distribution is used when there are a fixed number of trials ($n$) and more than two possible outcomes for each trial. $X_{i}$ represents the number of times a specific outcome occurs. Therefore, the mean, variance, and expected value of multinomial distributions are calculated for each $X_{i}$, not $X$.\n\n**Notation:** $X \\sim \\textrm{Multinomial}(n,p)$ or $X \\sim M(n,p)$\n\n**Parameters:**\n\n-   $n$ = number of trials\n-   $p$ = vector with probabilities of each outcome (expressed as $p_{1},...,p_{k}$ where $k$ = number of possible mutually exclusive outcomes)\n\n**Mean and Expected Value:** $E(X_{i}) = np_{i}$\n\n**Variance:** $\\textrm{Var}(X_{i}) = np_{i}(1-p_{i})$\n\n**PMF:** $P(X_{1}=x_{i},...,X_{k}=x_{k})=\\frac{n!}{x_{i}!...x_{k}!}p^{x_{1}}...p^{x_{k}}$\n\n**CDF:** COULDNT FIND\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** There is a candy jar consisting of 5 red candies, 3 blue candies, and 7 yellow candies.\n\n-   The probability of drawing a red candy is $\\frac{1}{3}$.\n\n-   The probability of drawing a blue candy is $\\frac{1}{5}$.\n\n-   The probability of drawing a yellow candy is $\\frac{7}{15}$\n\nYou draw 3 candies from the jar, replacing as you go along. This can be expressed as $X \\sim M(3,\\frac{1}{3},\\frac{1}{5},\\frac{7}{15})$. It means 3 trials are conducted, where $p_{1}=\\frac{1}{3}$, $p_{2}=\\frac{1}{5}$, and $p_{3}=\\frac{7}{15}$ (and $k=3$).\n\n## Poisson Distribution {.unnumbered}\n\n**Where to use:** The Poisson distribution is used when a specific event occurs at some rate $\\lambda$, and you are counting $X$, the number of times this event occurs in some interval.\n\n**Notation:** $X \\sim \\textrm{Poisson}(\\lambda)$ or $X \\sim \\textrm{Pois}(\\lambda)$\n\n**Parameter:** $\\lambda$ = number of times an event occurs within a specific period of time\n\n**Mean and Expected Value:** $E(X) = \\lambda$\n\n**Variance:** $\\textrm{Var}(X) = \\lambda$\n\n**PMF:** $P(X=x)=\\frac{\\lambda^xe^{-\\lambda}}{x!}$\n\n**CDF:** $P(X \\leq x)=\\sum^{\\lfloor x \\rfloor}_{i=1}\\frac{\\lambda^xe^{-\\lambda}}{x!}$ where $\\lfloor x \\rfloor$ is $x$ rounded down to the nearest integer\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** Customers enter Cantor’s Confectionery at an average rate of 20 people per hour. This can be expressed as $X \\sim \\textrm{Pois}(20)$, meaning the event of customers entering the store occurs 20 times within an hour.\n\n## Negative Binomial Distribution {.unnumbered}\n\n**Where to use:** The negative binomial distribution is often used to handle over-dispersed data, which means the variance exceeds the mean. It can serve as an alternative to the Poisson distribution, as the Poisson distribution assumes that the mean is equal to the variance. $X$ represents the number of trials required to reach the targeted number of successes $r$.\n\n**Notation:** $X \\sim \\textrm{NB}(r,p)$\n\n**Parameters:**\n\n-   $r$ = targeted number of successes\n-   $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Mean and Expected Value:** $E(X) = \\frac{r(1-p)}{p}$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{r(1-p)}{p^2}$\n\n**PMF:** $P(X=x)=\\frac{(x+r-1)!}{(r-1)!x!}(1-p)^xp^r$\n\n**CDF:** $P(X \\leq x)=\\sum^{x}_{i=1}\\frac{(x+r-1)!}{(r-1)!x!}(1-p)^xp^r$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You flip a coin multiple times, and the probability of getting ‘heads’ is 0.5. You decide to stop flipping the coin once you get 3 ‘heads’; these do not have to be consecutive. Taking ‘heads’ as a success, this can be expressed as $X \\sim \\textrm{NB}(3,0.5)$. It means the probability of success is 0.5, and you will stop conducting trials after you reach 3 successes.\n\n## Geometric Distribution {.unnumbered}\n\n**Where to use:** The geometric distribution is used to count $X$, the number of trials until a successful outcome is reached.\n\n**Notation:** $X \\sim \\textrm{Geometric}(p)$\n\n**Parameter:** $p$ = probability of success (where $0 \\le p \\le 1$)\n\n**Mean and Expected Value:** $E(X) = \\frac{1}{p}$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{1-p}{p^2}$\n\n**PMF:** $P(X=x)=(1-p)^{k-1}p$\n\n**CDF:** $P(X \\leq x)=\\begin{cases} 1-(1-p)^x & \\textsf{if } x\\geq0 \\\\0 & \\textsf{if } x<1\\end{cases}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You flip a coin multiple times, and the probability of getting ‘heads’ is 0.5. You decide to stop flipping the coin once you get a ‘heads’. Taking ‘heads’ as a success, this can be expressed as $X \\sim \\textrm{Geometric}(0.5)$. It means the probability of success is 0.5, and you will stop conducting trials after you reach a success.\n\n# Continuous Random Variables\n\n## Uniform Distribution (Continuous) {.unnumbered}\n\n**Where to use:** The continuous uniform distribution is used when all continuous values in the interval $a$ to $b$ are equally likely.\n\n**Notation:** $X \\sim \\textrm{Uniform}(a,b)$ or $X \\sim U(a,b)$\n\n**Parameters:**\n\n-   $a$ = minimum value\n-   $b$ = maximum value where all values of $X$ for $a \\leq x \\leq b$ are equally likely\n\n**Mean and Expected Value:** $E(X) = \\frac{a+b}{2}$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{(b-a)^2}{12}$\n\n**PDF:** $P(X=x)=\\begin{cases} \\frac{1}{b-a} & \\textsf{if } a \\leq x \\leq b \\\\0 & \\textsf{otherwise}\\end{cases}$\n\n**CDF:** $P(X\\leq x)= \\begin{cases} 0 & \\textsf{if } x< a \\\\\\frac{x-a}{b-a} & \\textsf{if } a\\leq x\\leq b \\\\1 & \\textsf{if } x>b \\end{cases}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:**\n\nA machine from Cantor's Confectionery is programmed to chop long candy bars into pieces, each with a length between 30 milimeters to 50 milimeters. Due to variations in the machine, each continuous value between this interval is equally likely. This can be expressed as $X \\sim U(30,50)$. It means 30 is the minimum value and 50 is the maximum value, where all continuous values of $X$ for $30 \\leq x \\leq 50$ are equally likely.\n\n## Normal Distribution {.unnumbered}\n\n**Where to use:** The normal distribution is used to model continuous random variables, which can include any positive or negative real values. The use of this distribution is often justified by the Central Limit Theorem: as the sample size increases, the distribution of sample means will resemble a normal distribution more and more closely.\n\n**Notation:** $X \\sim \\textrm{Normal}(\\mu,\\sigma^2)$ or $X \\sim N(\\mu,\\sigma^2)$\n\n**Parameters:**\n\n-   $\\mu$ = mean (location parameter)\n-   $\\sigma^2$ = variance (squared scale parameter), where $\\sigma$ = standard deviation (scale parameter)\n\n**Mean and Expected Value:** $E(X) = \\mu$\n\n**Variance:** $\\textrm{Var}(X) = \\sigma^2$\n\n**PDF:** $P(X=x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n\n**CDF:** $P(X\\leq x)=\\frac{1}{2}[1+\\textrm{erf}(\\frac{x-\\mu}{\\sigma\\sqrt{2}})]$ where $\\textrm{erf}(x)$ is the error function of $x$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** The lengths of chocolate bars produced by Cantor’s Confectionery follow a normal distribution with a mean of 5.6 inches and a standard deviation of 1.2. This can be expressed as $X \\sim N(5.6, 1.2^2)$, meaning the data is normally distributed, centered at 5.6 (location parameter) and stretched by 1.2 (scale parameter).\n\n## Lognormal Distribution {.unnumbered}\n\n**Where to use:** The lognormal distribution is used to model continuous random variables with values that are both real and non-negative, wherein the logarithms of these variables follow a normal distribution.\n\n**Notation:** $X \\sim \\textrm{Lognormal}(\\mu,\\sigma^2)$\n\n**Parameters:**\n\n-   $\\mu$ = logarithm of location parameter\n-   $\\sigma^2$ = logarithm of scale parameter\n\n**Mean and Expected Value:** $E(X) = \\textrm{exp}(\\mu+\\frac{\\sigma^2}{2})$ where $\\textrm{exp}(x)=e^x$\n\n**Variance:** $\\textrm{Var}(X) = [\\textrm{exp}(\\sigma^2)-1]\\textrm{exp}(2\\mu+\\sigma^2)$ where $\\textrm{exp}(x)=e^x$\n\n**PDF:** $P(X=x)=\\frac{1}{x\\sigma\\sqrt{2\\pi}}\\textrm{exp}(-\\frac{(\\textrm{ln}x-\\mu)^2}{2\\sigma^2})$ where $\\textrm{exp}(x)=e^x$ and $\\textrm{ln}x$ is the natural logarithm of $x$\n\n**CDF:** $P(X \\leq x)=\\frac{1}{2}[1+\\textrm{erf}(\\frac{\\textrm{ln}x-\\mu}{\\sigma\\sqrt{2}})]$ where $\\textrm{erf}(x)$ is the error function of $x$ and $\\textrm{ln}x$ is the natural logarithm of $x$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** The logarithms of Cantor’s Confectionery’s stock prices follow a normal distribution. The mean of the stock prices’ natural logarithms is 8.01, whereas the variance of the stock prices’ natural logarithms is 3. This can be expressed as $X \\sim \\textrm{Lognormal}(8.01, 3)$, meaning the logarithm of the location parameter is 8.01 and the logarithm of scale parameter is 3.\n\n## Exponential Distribution {.unnumbered}\n\n**Where to use:** The exponential distribution is used when $X$ is the waiting time before a certain event occurs. It is similar to the geometric distribution, but the exponential distribution uses continuous waiting time instead of the integer number of trials.\n\n**Notation:** $X \\sim \\textrm{Exponential}(\\lambda)$ or $X \\sim \\textrm{Exp}(\\lambda)$\n\n**Parameter:** $\\lambda$ = number of times an event occurs within a specific period of time\n\n**Mean and Expected Value:** $E(X) = \\frac{1}{\\lambda}$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{1}{\\lambda^2}$\n\n**PDF:** $P(X=x)=\\lambda e^{-\\lambda x}$\n\n**CDF:** $P(X \\leq x)=1-e^{-\\lambda x}$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** Customers enter Cantor’s Confectionery at an average rate of 20 people per hour, and the time distance between each visit can be modeled by an exponential distribution. This can be expressed as $X \\sim \\textrm{Exp}(20)$.\n\n## Gamma Distribution {.unnumbered}\n\n**Where to use:** The gamma distribution generalizes the exponential distribution, allowing for greater or lesser variance. It is used to model positive continuous random variables that have skewed distributions.\n\n**Notation:** $X \\sim \\textrm{Gamma}(\\alpha,\\theta)$ or $X \\sim \\textrm{Gam}(\\alpha,\\theta)$\n\n**Parameters:**\n\n-   $\\alpha = \\frac{\\mu^2}{\\sigma^2}$ (shape parameter)\n-   $\\theta=\\frac{\\sigma^2}{\\mu}$ (scale parameter) where $\\mu$ = mean and $\\sigma^2$ = variance\n\n**Mean and Expected Value:** $E(X) = \\alpha\\theta$\n\n**Variance:** $\\textrm{Var}(X) = \\alpha\\theta^2$\n\n**PDF:** $P(X=x)=\\frac{1}{\\Gamma(\\alpha)\\theta^{\\alpha}}x^{\\alpha-1}e^{-\\frac{x}{\\theta}}$ where $\\Gamma(x)$ is the gamma function of $x$\n\n**CDF:** $P(X \\leq x)=\\frac{1}{\\Gamma(\\alpha)}\\textrm{Gam}(\\alpha,\\frac{x}{\\theta})$ where $\\textrm{Gam}(\\alpha,\\theta)$ is the PDF of the gamma distribution for parameters $\\alpha$ and $\\theta$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You collect historical data on the time to failure of a machine from Cantor’s Confectionery. The mean is 83 days and the variance is 50.3. You can then use this to estimate the shape and scale parameters of the Gamma Distribution:\n\n-   $\\alpha = \\frac{83^2}{50.3} = 136.958250497 \\approx 137$\n\n-   $\\theta = \\frac{50.3}{83} = 0.60602409638 \\approx 0.61$\n\nThe distribution can be expressed as $X \\sim \\textrm{Gam}(137,0.61)$, where the shape parameter is 137 and the scale parameter is 0.61.\n\n## Beta Distribution {.unnumbered}\n\n**Where to use:** The beta distribution is used to model the distribution of probabilities or proportions. Hence, $0 \\leq X \\leq 1$.\n\n**Notation:** $X \\sim \\textrm{Beta}(\\alpha,\\beta)$\n\n**Parameters:**\n\n-   $\\alpha = k + 1$ (shape parameter)\n-   $\\beta = n - k + 1$ (shape parameter) where $k$ = number of successes and $n$ = number of trials\n\n**Mean and Expected Value:** $E(X) = \\frac{\\alpha}{\\alpha+\\beta}$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$\n\n**PDF:** $P(X=x)=\\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha,\\beta)}$ where $\\textrm{B}(x,y)$ is the beta function of $x$ and $y$\n\n**CDF:** $P(X \\leq x)=I_{x}(\\alpha,\\beta)$ where $I_{x}(a,b)$ is the regularized incomplete beta function for $a$ and $b$ with $x$ being the upper bound for integration\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** Cantor’s Confectionery is visited by 10 customers, and 6 of them purchase something from the store. Taking the buying customers as successes and the total visiting customers as number of trials, there would be 6 successes, allowing you to find the following parameters:\n\n-   $\\alpha = 6 + 1 = 7$\n\n-   $\\beta = 10 - 6 + 1 = 5$\n\nThen the distribution of the probabilities of a customer purchasing from Cantor’s Confectionery can be expressed as $X \\sim \\textrm{Beta}(7,5)$,meaning the first shape parameter is 7 and the second shape parameter is 5.\n\n## chi^2 Distribution {.unnumbered}\n\n**Where to use:** The $\\chi^2$ distribution is used for hypothesis testing, such as for goodness of fit tests and tests for independence. It is a special case of the gamma distribution, as $\\chi^2(k)=\\textrm{Gam}(\\frac{k}{2},2)$.\n\n**Notation:** $X \\sim \\chi^2(k)$\n\n**Parameter:** $k$ = degrees of freedom in sample\n\n**Mean and Expected Value:** $E(X) = k$\n\n**Variance:** $\\textrm{Var}(X) = 2k$\n\n**PDF:** $P(X=x)=\\frac{1}{2^\\frac{k}{2}\\Gamma(\\frac{k}{2})}x^{\\frac{k}{2}-1}e^{-\\frac{x}{2}}$ where $\\Gamma(x)$ is the gamma function of $x$\n\n**CDF:** $P(X \\leq x)=\\frac{1}{\\Gamma(\\frac{k}{2})}\\textrm{Gam}(\\frac{k}{2},\\frac{x}{2})$ where $\\Gamma(x)$ is the gamma function of $x$ and $\\textrm{Gam}(\\alpha,\\theta)$ is the PDF of the gamma distribution for parameters $\\alpha$ and $\\theta$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Examples:**\n\n-   **Goodness of Fit Example:** You have a six-sided die with six possible outcomes: 1, 2, 3, 4, 5, and 6. You calculate the expected frequencies of each outcome. Then you roll the die many times and record the observed frequencies of each outcome. Since there are 6 categories, $$\\textsf{degrees of freedom = number of categories} - 1 = 6 - 1 = 5$$ This can be expressed as $X \\sim \\chi^2(5)$, meaning the degrees of freedom is 5.\n\n-   **Test for Independence Example:** You are investigating whether there is a correlation between two variables: candy color and flavor. You have 5 categories of colors and 3 categories of flavors. Calculating the degrees of freedom can be done with: $$(\\textsf{number of rows} - 1)(\\textsf{number of columns} - 1) = (5-1)(3-1)=(4)(2)=8$$ Hence, $X \\sim \\chi^2(8)$, meaning the degrees of freedom is 8.\n\n## $F$ Distribution {.unnumbered}\n\n**Where to use:** The $F$ distribution is used for the ratio of two independent random $\\chi^2$ variables. It is commonly used as a reference distribution in hypothesis testing to compare two variances or more than two means, such as Analysis of Variance (ANOVA) tests.\n\n**Notation:** $X \\sim F(d_{1},d_{2})$\n\n**Parameters:**\n\n-   $d_{1}$ = numerator degrees of freedom\n-   $d_{2}$ = denominator degrees of freedom\n\n**Mean and Expected Value:** $E(X) = \\frac{d_{2}}{d_{2}-2}$ for $d_{2}>2$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{2d_{2}(d_{1}+d_{2}-2)}{d_{1}(d_{2}-2)^2(d_{2}-4)}$\n\n**PDF:** $P(X=x)=\\frac{\\sqrt{\\frac{(d_{1}x)^{d_{1}}d_{2}^{d_{2}}}{(d_{1}x+d_{2})^{d_{1}+d_{2}}}}}{x\\textrm{B}(\\frac{d_{1}}{2},\\frac{d_{2}}{2})}$ where $\\textrm{B}(x,y)$ is the beta function of $x$ and $y$\n\n**CDF:** $P(X \\leq x)=I_{\\frac{d_{1}x}{d_{1}x+d_{2}}}(\\frac{d_{1}}{2},\\frac{d_{2}}{2})$ where $I_{x}(a,b)$ is the regularized incomplete beta function for $a$ and $b$ with $x$ being the upper bound for integration\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You have three independent groups of data containing Cantor’s Confectionery chocolate bar lengths, and the total sample size is 90. From this, you would like to conduct an ANOVA test investigating if there is a statistically significant difference between each group’s means. You find the degrees of freedom using the following methods:\n\n-   $\\textsf{numerator degrees of freedom = number of groups} - 1 = 3 - 1 = 2$\n\n-   $\\textsf{denominator degrees of freedom = sample size - number of groups} = 90 - 3 = 87$\n\nThe $F$ distribution, which will be used as a reference distribution for the ANOVA test, can be expressed as $X \\sim F(2,87)$, meaning the numerator degrees of freedom is 2 and the denominator degrees of freedom is 87.\n\n## $t$ Distribution {.unnumbered}\n\n**Where to use:** The $t$ distribution is a special case of the $F$ distribution, as $(t(v))^2 = F(1,v)$. This distribution is used for continuous random variables with heavier tails than the normal distribution, and it is often employed in hypothesis testing when the population standard deviation is unknown.\n\n**Notation:** $X \\sim t(v)$\n\n**Parameter:** $v$ = degrees of freedom (equal to denominator degrees of freedom if related to $F$ distribution)\n\n**Mean and Expected Value:** $E(X) = 0$\n\n**Variance:** $\\textrm{Var}(X) = \\frac{v}{v-2}$ for $v>2$\n\n**PDF:** $P(X=x)=\\frac{\\Gamma(\\frac{v+1}{2})}{\\sqrt{\\pi v}\\Gamma(\\frac{v}{2})}(1+\\frac{x^2}{v})^{-\\frac{v+1}{2}}$ where $\\Gamma(x)$ is the gamma function of $x$\n\n**CDF:** $P(X \\leq x)=\\frac{1}{2}+x\\Gamma(\\frac{v+1}{2})(\\frac{_{2}F_{1}(\\frac{1}{2},\\frac{v+1}{2};\\frac{3}{2};-\\frac{x^2}{v})}{\\sqrt{\\pi v}\\Gamma(\\frac{v}{2})})$ where $\\Gamma(x)$ is the gamma function of $x$ and $_{2}F_{1}(a,b;c;z)$ is the hypergeometric function for $a, b, c,$ and $z$\n\n**Figure**\n\nPUT FIGURE HERE TOM\n\n**Example:** You have a sample of 40 measurements of Cantor’s Confectionery chocolate bar lengths. From this, you would like to conduct a one sample t-test comparing the sample to a hypothesized mean. You find the degrees of freedom:\n\n$$\n\\textsf{degrees of freedom = sample size} - 1 = 40 - 1 = 39\n$$\n\nThe $t$ distribution, which will be used as a reference distribution for the t-test, can be expressed as $X \\sim t(39)$, meaning the degrees of freedom is 39.\n\n# Further reading {.unnumbered}\n\n\\[For more information on hypothesis testing, please see Guide: Introduction to hypothesis testing.\\]\n\n\\[For more information on probability, please see Guide: Introduction to Probability.\\]\n\n\\[For more information on mean, variance, and standard deviation, please see Guide: Mean, Variance, and Standard Deviation.\\]\n\n\\[For more information on PMFs, PDFs, and CDFs, please see Guide: PMFs, PDFs, and CDFs.\\]\n\n## Version history {.unnumbered}\n\nv1.0: initial version created 4/25 by Michelle Arnetta as part of a University of St Andrews VIP project.\n\n[This work is licensed under CC BY-NC-SA 4.0.](https://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}